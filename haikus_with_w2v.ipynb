{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cfb0df2-6691-42f0-a537-06a1ffbfd9a7",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc7d1ec-28ab-4353-9de5-9d0102b0d061",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "203148e5-af60-4988-b678-bda538d6249b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-07 14:41:39.936445: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-07 14:41:39.936508: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "from tqdm import tqdm\n",
    "import gensim\n",
    "from keras.callbacks import LambdaCallback\n",
    "\n",
    "# Deep learning: \n",
    "from keras.models import Input, Model\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d02a6e-d7e4-49bb-85a8-db2d70189bb6",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c39059e-77d5-42d6-bdce-766e49e9f5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datensatz einlesen\n",
    "df = pd.read_csv('data/big_out.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbd8388-e4bc-4b0b-b6f2-c8ab37360f92",
   "metadata": {},
   "source": [
    "## Transform dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f19816b9-d18a-4c52-bda3-7aef12cb0867",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataframe mit 3 Spalten. Werden so gejoint, dass ein neues Dataframe mit ein Haiku pro Zeile erstellt wird\n",
    "df = df[['0', '1', '2']].agg(lambda x: ' \\n '.join(x.values), axis=1)\n",
    "# Dataframe to list [[]] -> []\n",
    "haikus = df.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2726013-e79e-4d25-b5da-a0fc4e2c45f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['last red in the sky \\n a small girls moon face rises \\n over the counter', 'christmas services \\n a cellular phone rings out \\n handels messiah', 'passover darkness  \\n before the buds burst open \\n a childs eyes in death', 'last night of summer \\n the bright full moon of last night \\n hidden by a cloud', 'midnight and full moon \\n my neighbour asks to borrow \\n the vacum cleaner', 'yellow walnut leaves \\n slowly appear on the lawn \\n early morning light', 'after its first flight \\n the young gerfalcons talons \\n tighter on my glove', 'sultry afternoon \\n only the mailbox shadow \\n crosses the dirt road', 'long journey back home  \\n a forgotten bale of hay \\n slowly rots away', 'autumn mist obscures \\n the island in the distance \\n she cleans her glasses']\n",
      "number of haikus: 10000\n"
     ]
    }
   ],
   "source": [
    "# alle Haikus in Array\n",
    "print(haikus[:10])\n",
    "#number_of_haikus = len(haikus)\n",
    "number_of_haikus = 10000\n",
    "\n",
    "haikus = haikus[:number_of_haikus]\n",
    "print('number of haikus: ' + str(number_of_haikus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94a0b771-a466-4239-95a2-db1b428b1c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# die Haikus cleanen und selber auch noch mal als Wort-Arrays in großen Array\n",
    "haikus = np.array(haikus)\n",
    "\n",
    "def clean_and_split(sentence):\n",
    "    result = list(filter(''.__ne__, re.sub('[.,_]', '', sentence).split(' ')))\n",
    "    result.append(';')\n",
    "    return result\n",
    "\n",
    "haikus = list(map(lambda x: clean_and_split(x), haikus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bac24142-4e2f-43fd-aeea-6c8d3cf31769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['last', 'red', 'in', 'the', 'sky', '\\n', 'a', 'small', 'girls', 'moon', 'face', 'rises', '\\n', 'over', 'the', 'counter', ';'], ['christmas', 'services', '\\n', 'a', 'cellular', 'phone', 'rings', 'out', '\\n', 'handels', 'messiah', ';'], ['passover', 'darkness', '\\n', 'before', 'the', 'buds', 'burst', 'open', '\\n', 'a', 'childs', 'eyes', 'in', 'death', ';'], ['last', 'night', 'of', 'summer', '\\n', 'the', 'bright', 'full', 'moon', 'of', 'last', 'night', '\\n', 'hidden', 'by', 'a', 'cloud', ';'], ['midnight', 'and', 'full', 'moon', '\\n', 'my', 'neighbour', 'asks', 'to', 'borrow', '\\n', 'the', 'vacum', 'cleaner', ';'], ['yellow', 'walnut', 'leaves', '\\n', 'slowly', 'appear', 'on', 'the', 'lawn', '\\n', 'early', 'morning', 'light', ';'], ['after', 'its', 'first', 'flight', '\\n', 'the', 'young', 'gerfalcons', 'talons', '\\n', 'tighter', 'on', 'my', 'glove', ';'], ['sultry', 'afternoon', '\\n', 'only', 'the', 'mailbox', 'shadow', '\\n', 'crosses', 'the', 'dirt', 'road', ';'], ['long', 'journey', 'back', 'home', '\\n', 'a', 'forgotten', 'bale', 'of', 'hay', '\\n', 'slowly', 'rots', 'away', ';'], ['autumn', 'mist', 'obscures', '\\n', 'the', 'island', 'in', 'the', 'distance', '\\n', 'she', 'cleans', 'her', 'glasses', ';']]\n"
     ]
    }
   ],
   "source": [
    "print(haikus[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb02a0c-f7c6-4a67-b5c5-4fa75c5ab519",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## The word_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2343e975-ba2e-4874-a953-1ec4242fd5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train word_model\n",
    "word_model = gensim.models.Word2Vec(haikus, min_count=1) # min count automatisch groesser, discarded alle woerter die weniger vorkommen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d491045-c758-4929-aaf2-4cf375433183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec<vocab=11016, vector_size=100, alpha=0.025>\n"
     ]
    }
   ],
   "source": [
    "# summarize the loaded word_model\n",
    "print(word_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd1d78a9-21c3-4ce8-b2fa-3e8d6886af63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector for 'girl':\n",
      "[ 1.57944846e-03  4.79702383e-01  1.39830023e-01  2.32443362e-01\n",
      " -2.48008892e-01 -4.77285028e-01  1.59128159e-01  4.97037202e-01\n",
      " -4.14219916e-01 -4.56947803e-01 -1.19305857e-01 -1.13006361e-01\n",
      " -4.89470316e-03  5.25909364e-02  1.62677288e-01 -2.62931585e-01\n",
      "  2.48516634e-01 -2.77941346e-01 -3.44668418e-01 -6.84914947e-01\n",
      "  6.54398231e-03 -3.69090326e-02  3.39929581e-01 -1.39447480e-01\n",
      " -5.92733435e-02 -4.46793949e-03 -1.83766633e-01  5.40932901e-02\n",
      " -2.79231161e-01  1.23383693e-01  9.54838190e-03 -2.06339657e-01\n",
      "  2.08209325e-02 -3.76382232e-01 -1.11764356e-01  1.22187719e-01\n",
      "  1.89152375e-01 -1.48662120e-01 -1.02862649e-01 -4.83266234e-01\n",
      " -1.68181062e-01 -1.70294326e-04 -3.38109463e-01  2.04289779e-01\n",
      "  1.06395714e-01 -1.53483137e-01 -2.15118691e-01  1.29657567e-01\n",
      "  1.43383488e-01  4.54440027e-01  4.59779613e-02 -4.42435622e-01\n",
      " -1.20817512e-01 -1.09769121e-01 -1.77901015e-01  6.78219274e-02\n",
      " -1.15541734e-01 -8.38335380e-02 -5.88402115e-02  3.26623842e-02\n",
      "  5.66705056e-02 -1.59036607e-01  1.24281317e-01  5.73311225e-02\n",
      " -2.48219788e-01  4.98570055e-01  4.88298461e-02  4.06385899e-01\n",
      " -3.83060247e-01  2.93361455e-01  9.40276310e-02  3.77597004e-01\n",
      "  3.77202034e-01 -1.35221723e-02 -2.60630678e-02  2.88135022e-01\n",
      "  1.40836775e-01  9.87609625e-02 -1.93936765e-01 -1.78966627e-01\n",
      " -1.07464887e-01  9.57861096e-02 -1.80226907e-01  2.74383962e-01\n",
      "  4.44351174e-02 -4.05830331e-02 -5.40812733e-03  1.01188026e-01\n",
      "  3.16125572e-01  3.93702760e-02  2.83947647e-01  1.25908241e-01\n",
      " -1.18308507e-01  9.09901783e-02  4.04006064e-01 -2.11081319e-02\n",
      "  1.01234950e-01 -4.72560465e-01  2.11980283e-01 -1.08933441e-01]\n"
     ]
    }
   ],
   "source": [
    "# access vector for one word\n",
    "print('vector for \\'girl\\':')\n",
    "print(word_model.wv['girl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fd3d026-5e46-405a-ae8e-ebab45ee2075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10 words most similar to 'girl':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('baby', 0.9995694756507874),\n",
       " ('again', 0.9995231628417969),\n",
       " ('then', 0.9994540810585022),\n",
       " ('him', 0.9993935227394104),\n",
       " ('still', 0.9993851184844971),\n",
       " ('she', 0.9993551969528198),\n",
       " ('cause', 0.9993375539779663),\n",
       " ('before', 0.9993357062339783),\n",
       " ('does', 0.9993078708648682),\n",
       " ('everyone', 0.9993048310279846)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('top 10 words most similar to \\'girl\\':')\n",
    "word_model.wv.most_similar('girl', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8ef099b-640b-4e25-99a0-6468adce2efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity between 'go' and 'walk' (regarding the haikus):\n",
      "0.9801817\n",
      "\n",
      "similarity between 'go' and 'laugh' (regarding the haikus):\n",
      "0.9883106\n",
      "\n",
      "similarity between 'go' and 'go':\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# similarity between two words\n",
    "print('similarity between \\'go\\' and \\'walk\\' (regarding the haikus):')\n",
    "print(word_model.wv.similarity(w1='go', w2='walk'))\n",
    "print()\n",
    "\n",
    "print('similarity between \\'go\\' and \\'laugh\\' (regarding the haikus):')\n",
    "print(word_model.wv.similarity(w1='go', w2='laugh'))\n",
    "print()\n",
    "\n",
    "print('similarity between \\'go\\' and \\'go\\':')\n",
    "print(word_model.wv.similarity(w1='go', w2='go'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef287aca-158d-47aa-a2ea-97b801155496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save word_model\n",
    "#word_model.save('w2v_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "108a5499-de48-4826-9fda-37a5cd5d8f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "#new_model = Word2Vec.load('w2v_model.bin')\n",
    "#print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb90c737-8d1a-4706-b013-f2a84b5950b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectors:\n",
      "[[-0.02006438  1.5637118   0.6815313   0.95296234 -1.0103472  -1.6432259\n",
      "   0.38379577  1.4681886  -1.299237   -1.2814176  -0.27528042 -0.35464862\n",
      "   0.12460577  0.01586231  0.42715144 -0.6344724   0.667469   -0.60655695\n",
      "  -1.0345472  -1.8220502   0.04231731  0.14243126  0.68242687 -0.34836373\n",
      "  -0.18041295  0.21925704 -0.5731937   0.28884244 -1.1853275   0.53131753\n",
      "  -0.41879782 -0.67153263 -0.09016158 -0.8381077  -0.5402491   0.27035642\n",
      "   0.7001314  -0.6446366  -0.029839   -1.56917    -0.8088404   0.07606144\n",
      "  -1.123368    0.4198324   0.26193577 -0.35734543 -0.6477      0.6020401\n",
      "   0.42673004  1.7353376  -0.00772176 -1.2835473  -0.42641953 -0.34523803\n",
      "  -0.41307467  0.28775847 -0.34370133  0.01172935 -0.25937247  0.42898273\n",
      "  -0.1205273  -0.21848534 -0.12997341  0.02379183 -0.6135289   1.2666781\n",
      "   0.27158913  1.2231308  -0.8192191   1.0312066   0.32499903  0.9899771\n",
      "   1.0708064  -0.29692984 -0.1008736   0.93253887  0.4496775   0.09123478\n",
      "  -0.4988745  -0.46505553 -0.25702325  0.33873042 -0.19072782  0.8033879\n",
      "   0.11732984  0.16711245 -0.36452374  0.0287909   0.92113423 -0.1000419\n",
      "   0.5078752   0.14285438 -0.19853295  0.23681426  1.0650998  -0.05276098\n",
      "   0.12015337 -1.5154581   0.60665816 -0.15469626]\n",
      " [-0.20071605  1.5030495   0.43181446  0.81843257 -0.78465366 -1.3227795\n",
      "   0.29690102  1.2708632  -1.4462768  -1.3686603  -0.5226026  -0.41462195\n",
      "   0.03589254  0.09624545  0.5694864  -0.6755172   0.7558011  -0.6585473\n",
      "  -0.86556625 -1.7520082  -0.20441064 -0.08472323  0.86384535 -0.5818171\n",
      "  -0.0740793   0.15238403 -0.6439775   0.21895847 -0.9618518   0.4286294\n",
      "  -0.11987019 -0.7281838  -0.26888737 -0.9340913  -0.40880796  0.32120925\n",
      "   0.4461094  -0.32966924 -0.10495323 -1.5339106  -0.50616056 -0.01703737\n",
      "  -1.1641821   0.76165587  0.19295502 -0.4216556  -0.5456943   0.3970913\n",
      "   0.44993073  1.3999225   0.15417027 -1.4711723  -0.3371098  -0.3171342\n",
      "  -0.5089073   0.40724018 -0.36578935 -0.19963975 -0.30634877  0.0621088\n",
      "   0.07744421 -0.33493882  0.24594483  0.16576837 -0.7140639   1.4708346\n",
      "   0.23358567  1.1865479  -0.82115316  0.91496104  0.3175178   1.0770041\n",
      "   1.1845952  -0.03513617 -0.21819593  1.025798    0.35917324  0.11830001\n",
      "  -0.509887   -0.5498756  -0.4305571   0.32179287 -0.2002986   0.6957299\n",
      "   0.25483787 -0.13440533 -0.35572606  0.19619797  1.1351533  -0.08653086\n",
      "   0.73389685  0.29779604 -0.3429452   0.2726931   1.1537012  -0.12614553\n",
      "   0.31485957 -1.3839233   0.61108494 -0.2848484 ]]\n",
      "\n",
      "labels:\n",
      "['\\n' ';' 'the' 'i' 'to' 'a' 'you' 'and' 'my' 'is']\n"
     ]
    }
   ],
   "source": [
    "# extract the words & their vectors, as numpy arrays\n",
    "vectors = np.asarray(word_model.wv.vectors)\n",
    "labels = np.asarray(word_model.wv.index_to_key)  # fixed-width numpy strings\n",
    "\n",
    "print('vectors:')\n",
    "print(vectors[:2])\n",
    "print()\n",
    "print('labels:')\n",
    "print(labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa22bfde-7705-43b1-a949-9610f087d26a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11016"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fde1864f-9255-4257-b256-3393ca0d312d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11016"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbf6e4f2-1ec2-4371-8158-7fb7879527d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://projector.tensorflow.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a668015a-c472-4633-b2f2-2b134fa7c817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metadata (labels) into tsv file\n",
    "pd.DataFrame(labels).to_csv(\"model_dir/metadata.tsv\", sep = '\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5e66453-e7ed-49b0-99c3-7cbb96887765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save vectors into tsv file\n",
    "pd.DataFrame(vectors).to_csv(\"model_dir/vectors.tsv\", sep = '\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962ce0f1-52c7-4c0f-986e-3dc9b46dec9b",
   "metadata": {},
   "source": [
    "## Creating Model for HaikuGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b257feb1-587f-416d-8629-8794d3f8c05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximale Anzahl der Wörter in einem Haiku aus Datenset\n",
    "max_haiku_len = len(max(haikus, key=len))\n",
    "max_features = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ca2faec-c14d-49e3-b4e8-a91829318694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['midnight',\n",
       " 'and',\n",
       " 'full',\n",
       " 'moon',\n",
       " '\\n',\n",
       " 'my',\n",
       " 'neighbour',\n",
       " 'asks',\n",
       " 'to',\n",
       " 'borrow',\n",
       " '\\n',\n",
       " 'the',\n",
       " 'vacum',\n",
       " 'cleaner',\n",
       " ';']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "haikus[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24c29a53-dd37-4eab-b5c8-0786d1166a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.11142737,  1.3569297 ,  0.3834107 ,  0.68843246, -0.7094674 ,\n",
       "       -1.2384541 ,  0.34475657,  1.2290188 , -1.216967  , -1.280157  ,\n",
       "       -0.36968192, -0.3127435 ,  0.03081723,  0.08334462,  0.4768745 ,\n",
       "       -0.6999552 ,  0.69164765, -0.69219   , -0.84108776, -1.7297027 ,\n",
       "       -0.11817645, -0.0819692 ,  0.84486747, -0.4489803 , -0.08038399,\n",
       "        0.06905802, -0.55460745,  0.17209111, -0.7952637 ,  0.3244609 ,\n",
       "       -0.04219412, -0.5925541 , -0.09798866, -0.9510048 , -0.36299366,\n",
       "        0.3249889 ,  0.42437774, -0.34720448, -0.1957351 , -1.3518991 ,\n",
       "       -0.4604878 ,  0.01482105, -0.9969884 ,  0.60189366,  0.2684482 ,\n",
       "       -0.4044062 , -0.51662916,  0.30371323,  0.42953113,  1.2160685 ,\n",
       "        0.09136987, -1.2464862 , -0.31920072, -0.3307937 , -0.4878993 ,\n",
       "        0.21514072, -0.36327156, -0.26939368, -0.20047665,  0.04584768,\n",
       "        0.17755835, -0.40301216,  0.34076568,  0.18358415, -0.67770576,\n",
       "        1.36355   ,  0.15298508,  1.0931408 , -0.891092  ,  0.81785077,\n",
       "        0.29803967,  1.0646155 ,  1.0699034 , -0.01859787, -0.08057845,\n",
       "        0.88481617,  0.31986877,  0.20049076, -0.50044966, -0.54568154,\n",
       "       -0.28773358,  0.22814126, -0.36906475,  0.6918219 ,  0.1813006 ,\n",
       "       -0.18492371, -0.1307964 ,  0.18455556,  0.88197553,  0.00218997,\n",
       "        0.75561917,  0.32584748, -0.30050504,  0.24923818,  1.0865233 ,\n",
       "       -0.0767393 ,  0.22899824, -1.2053226 ,  0.5419798 , -0.3215671 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_model.wv[\"out\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f4d81a12-52e9-460b-9de6-b13a56a8b1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2idx(word):\n",
    "    return word_model.wv.key_to_index[word]\n",
    "def idx2word(idx):\n",
    "    return word_model.wv.index_to_key[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d5ef81-7dd6-457c-9a23-77b8bc7523c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d63714d6-b354-48c2-bce6-b322b37de08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 52714\n",
      "train_x shape: (52714, 20)\n",
      "train_y shape: (52714,)\n"
     ]
    }
   ],
   "source": [
    "haikus_combined =  sum(haikus, [])\n",
    "# cut the text in semi-redundant sequences of seq_len characters\n",
    "seq_len = max_haiku_len\n",
    "step = 3\n",
    "# Input String\n",
    "sequences = []\n",
    "#Output character\n",
    "next_words = []\n",
    "for i in range(0, len(haikus_combined) - seq_len, step):\n",
    "    sequences.append(haikus_combined[i : i + seq_len])\n",
    "    next_words.append(haikus_combined[i + seq_len])\n",
    "print(\"Number of sequences:\", len(sequences))\n",
    "\n",
    "train_x = np.zeros([len(sequences), max_haiku_len], dtype=np.int32)\n",
    "train_y = np.zeros([len(next_words)], dtype=np.int32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, word in enumerate(sequence):\n",
    "         train_x[i, t] = word2idx(word)\n",
    "    train_y[i] = word2idx(next_words[i])\n",
    "print('train_x shape:', train_x.shape)\n",
    "print('train_y shape:', train_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "005209b8-d549-465b-9c84-be781600d359",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('\\nPreparing the data for LSTM...')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#train_x = np.zeros([len(haikus), max_haiku_len], dtype=np.int32)\n",
    "#train_y = np.zeros([len(haikus)], dtype=np.int32)\n",
    "#for i, haiku in enumerate(haikus):\n",
    "#    for t, word in enumerate(haiku[:-1]):\n",
    "#        train_x[i, t] = word2idx(word)\n",
    "#    train_y[i] = word2idx(haiku[-1])\n",
    "#print('train_x shape:', train_x.shape)\n",
    "#print('train_y shape:', train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2dd02369-d7cd-4e41-9220-79c229c8fb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result embedding shape: (11016, 100)\n"
     ]
    }
   ],
   "source": [
    "pretrained_weights = word_model.wv.vectors\n",
    "vocab_size = pretrained_weights.shape[0]\n",
    "emdedding_size = pretrained_weights.shape[1]\n",
    "print('Result embedding shape:', pretrained_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "121ed410-c97d-47cf-849d-9464ccee1020",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Activation \n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c62ea891-6506-4d4c-8b83-6733a6a14d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-07 14:41:58.534649: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-06-07 14:41:58.534760: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-06-07 14:41:58.534812: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (jupyter-acq716): /proc/driver/nvidia/version does not exist\n",
      "2022-06-07 14:41:58.535175: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=emdedding_size, weights=[pretrained_weights]))\n",
    "model.add(LSTM(emdedding_size, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(vocab_size)),\n",
    "model.add(Activation('softmax')),\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "#print('\\nTraining LSTM...')\n",
    "#model = Sequential()\n",
    "#model.add(Embedding(input_dim=vocab_size, output_dim=emdedding_size, weights=[pretrained_weights]))\n",
    "#model.add(LSTM(units=emdedding_size))\n",
    "#model.add(Dense(units=vocab_size))\n",
    "#model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "13eacbe7-335b-4dcb-a9f9-aaa8093eba92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 100)         1101600   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               80400     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 11016)             1112616   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 11016)             0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,294,616\n",
      "Trainable params: 2,294,616\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "926f1a9c-643c-45d0-be80-54aead671437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "103/103 [==============================] - 66s 621ms/step - loss: 6.8344\n",
      "Epoch 2/5\n",
      "103/103 [==============================] - 64s 618ms/step - loss: 6.1007\n",
      "Epoch 3/5\n",
      "103/103 [==============================] - 64s 624ms/step - loss: 6.0706\n",
      "Epoch 4/5\n",
      "103/103 [==============================] - 63s 614ms/step - loss: 6.0080\n",
      "Epoch 5/5\n",
      "103/103 [==============================] - 63s 616ms/step - loss: 5.9291\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa5d6db22e0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x, train_y,\n",
    "          batch_size=512,\n",
    "          epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "127bd25e-7303-4e8d-9807-8d9b014e4f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "  if temperature <= 0:\n",
    "    return np.argmax(preds)\n",
    "  preds = np.asarray(preds).astype('float64')\n",
    "  preds = np.log(preds) / temperature\n",
    "  exp_preds = np.exp(preds)\n",
    "  preds = exp_preds / np.sum(exp_preds)\n",
    "  probas = np.random.multinomial(1, preds, 1)\n",
    "  return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "40f48de5-a9e5-442e-af79-24436044a788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next(text, num_generated=10):\n",
    "  word_idxs = [word2idx(word) for word in text.lower().split()]\n",
    "  for i in range(num_generated):\n",
    "    prediction = model.predict(x=np.array(word_idxs))\n",
    "    idx = sample(prediction[-1], temperature=0.7)\n",
    "    word_idxs.append(idx)\n",
    "  return ' '.join(idx2word(idx) for idx in word_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db2aeea-8a22-416d-893f-ba57b694bba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_words = 50\n",
    "temperature = 1.0\n",
    "start_index = random.randint(0, train_x.size - seq_len - 1)\n",
    "generated = \"\"\n",
    "\n",
    "seed =  train_x[start_index:start_index+1]\n",
    "print(seed.shape)\n",
    "\n",
    "prediction = model.predict(x=np.array(word_idxs))\n",
    "    idx = sample(prediction[-1], temperature=temperature)\n",
    "    word_idxs.append(idx)\n",
    "\n",
    "preds = model.predict(train_x[:1]) # input shape(1, 20)\n",
    "\n",
    "\n",
    "\n",
    "print(preds.shape)\n",
    "\n",
    "print(sample(preds[-1]))\n",
    "\n",
    "print(idx2word(sample(preds[-1])))\n",
    "\n",
    "starts = [\n",
    "    'deep convolutional',\n",
    "    'simple and effective',\n",
    "    'a nonconvex',\n",
    "]\n",
    "\n",
    "for start in starts:\n",
    "    sample = generate_next(start)\n",
    "    print('%s... -> %s' % (start, sample))\n",
    "\n",
    "\n",
    "#for i in range(generate_words):\n",
    "#    x_pred = np.zeros((1, len(seed)))\n",
    "#    for t, char in enumerate(seed):\n",
    "#        x_pred[0, t, char_indices[char]] = 1\n",
    "#    preds = model.predict(x_pred, verbose=0)[0]\n",
    "#        \n",
    "#    next_index = sample(preds, temperature)\n",
    "#    next_char = indices_char[next_index]\n",
    "#    seed = seed[1:] + next_char\n",
    "#    generated += next_char\n",
    "        \n",
    "#    if next_char == \";\":\n",
    "#            generated += \"\\n----------------------------------------\\n\"\n",
    "            \n",
    "#print(generated) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12613068-af8c-454b-99d3-d9c325b6b68d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c929fa-14f7-45f3-9357-47b521859eb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
