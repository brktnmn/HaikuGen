{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cfb0df2-6691-42f0-a537-06a1ffbfd9a7",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc7d1ec-28ab-4353-9de5-9d0102b0d061",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "203148e5-af60-4988-b678-bda538d6249b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-23 13:07:28.206066: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-23 13:07:28.206107: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "from tqdm import tqdm\n",
    "from scipy import sparse\n",
    "\n",
    "# Deep learning: \n",
    "from keras.models import Input, Model\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29072d0f-c7ac-47f2-83f5-f809edd42372",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activate tensorboard extension\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "#Import necessary libs\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorboard.plugins import projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4acdb5f-346c-45c3-afcb-b8af2ec10aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-23 13:07:31.407232: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-05-23 13:07:31.407282: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-23 13:07:31.407308: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (jupyter-acs277): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d02a6e-d7e4-49bb-85a8-db2d70189bb6",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c39059e-77d5-42d6-bdce-766e49e9f5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datensatz einlesen\n",
    "df = pd.read_csv('data/out.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbd8388-e4bc-4b0b-b6f2-c8ab37360f92",
   "metadata": {},
   "source": [
    "## Transform dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f19816b9-d18a-4c52-bda3-7aef12cb0867",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataframe mit 3 Spalten. Werden so gejoint, dass ein neues Dataframe mit ein Haiku pro Zeile erstellt wird\n",
    "df = df[['0', '1', '2']].agg(lambda x: ' '.join(x.values), axis=1)\n",
    "# Dataframe to list [[]] -> []\n",
    "haikus = df.values.tolist()\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Erst mal nur mit 1000 Haikus!\n",
    "haikus = haikus[:1000]\n",
    "# ---------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2726013-e79e-4d25-b5da-a0fc4e2c45f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['why are you letting  your infant have table food so early stop please', 'never trust virgin  east coast they dont care unless u are elderly', 'paces wrigley field  express is a game changer and a life saver', 'now that the secrets  out i can stop living this dumb secretive life', 'everyday i thank  god for a roof over my head and my bills paid', 'nobody talk to  me for the rest of the day im going to sleep', 'i found the cutest  set from vs i hope its in leeds tomorrow', 'bother me tell me  awful things you know i love it when you do that', 'walmart kid doesnt  even tap his foot in time and that is the tea', 'me when people ask  if i got highlights but ive never dyed my hair']\n",
      "number of haikus: 1000\n"
     ]
    }
   ],
   "source": [
    "# alle Haikus in Array\n",
    "print(haikus[-10:])\n",
    "number_of_haikus = len(haikus)\n",
    "print('number of haikus: ' + str(number_of_haikus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34535e9-892b-41b4-b2cb-a89c60fdfcfb",
   "metadata": {},
   "source": [
    "## Clean text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05d688c4-b56f-46fc-a88c-862b2ef63d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "huge_string = ' SATZENDE SATZANFANG '.join(haikus)\n",
    "huge_string_cleaned = re.sub('[.,_]', '', huge_string)\n",
    "\n",
    "# TODO remove stopwords!\n",
    "\n",
    "huge_list = huge_string_cleaned.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f7bbaa9-0dd9-4e40-ba91-a19200826abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'SATZANFANG', 'SATZENDE', 'a', 'aaron', 'abandon', 'abandoning', 'abbey', 'aboating', 'abound', 'about', 'above', 'absent', 'absolute', 'absolutely', 'abstract', 'accepted', 'according', 'acrid', 'across']\n",
      "3454\n"
     ]
    }
   ],
   "source": [
    "#Get vocabulary\n",
    "vocab = sorted(set(huge_list))\n",
    "print(vocab[:20])\n",
    "vocab_size = len(vocab)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ab98fdc-692c-4c25-b965-ebe4831ee1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.array(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc70bbf-d679-4605-9da5-a35f29f57a1b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Create context windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29c70db9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Defining the window for context\n",
    "window = 3\n",
    "\n",
    "word_lists = []\n",
    "\n",
    "# Creating a context dictionary\n",
    "for i, word in enumerate(huge_list):\n",
    "    for w in range(window):\n",
    "        # Getting the context that is ahead by *window* words\n",
    "        if i + 1 + w < len(huge_list): \n",
    "            word_lists.append([word] + [huge_list[(i + 1 + w)]])\n",
    "        # Getting the context that is behind by *window* words    \n",
    "        if i - w - 1 >= 0:\n",
    "            word_lists.append([word] + [huge_list[(i - w - 1)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcd757b2-c8b1-41ea-901a-967cfbfb334e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['tea', 'SATZANFANG'], ['tea', 'is'], ['tea', 'me'], ['tea', 'that'], ['SATZENDE', 'SATZANFANG'], ['SATZENDE', 'tea'], ['SATZENDE', 'me'], ['SATZENDE', 'the'], ['SATZENDE', 'when'], ['SATZENDE', 'is'], ['SATZANFANG', 'me'], ['SATZANFANG', 'SATZENDE'], ['SATZANFANG', 'when'], ['SATZANFANG', 'tea'], ['SATZANFANG', 'people'], ['SATZANFANG', 'the'], ['me', 'when'], ['me', 'SATZANFANG'], ['me', 'people'], ['me', 'SATZENDE'], ['me', 'ask'], ['me', 'tea'], ['when', 'people'], ['when', 'me'], ['when', 'ask'], ['when', 'SATZANFANG'], ['when', ''], ['when', 'SATZENDE'], ['people', 'ask'], ['people', 'when'], ['people', ''], ['people', 'me'], ['people', 'if'], ['people', 'SATZANFANG'], ['ask', ''], ['ask', 'people'], ['ask', 'if'], ['ask', 'when'], ['ask', 'i'], ['ask', 'me'], ['', 'if'], ['', 'ask'], ['', 'i'], ['', 'people'], ['', 'got'], ['', 'when'], ['if', 'i'], ['if', ''], ['if', 'got'], ['if', 'ask'], ['if', 'highlights'], ['if', 'people'], ['i', 'got'], ['i', 'if'], ['i', 'highlights'], ['i', ''], ['i', 'but'], ['i', 'ask'], ['got', 'highlights'], ['got', 'i'], ['got', 'but'], ['got', 'if'], ['got', 'ive'], ['got', ''], ['highlights', 'but'], ['highlights', 'got'], ['highlights', 'ive'], ['highlights', 'i'], ['highlights', 'never'], ['highlights', 'if'], ['but', 'ive'], ['but', 'highlights'], ['but', 'never'], ['but', 'got'], ['but', 'dyed'], ['but', 'i'], ['ive', 'never'], ['ive', 'but'], ['ive', 'dyed'], ['ive', 'highlights'], ['ive', 'my'], ['ive', 'got'], ['never', 'dyed'], ['never', 'ive'], ['never', 'my'], ['never', 'but'], ['never', 'hair'], ['never', 'highlights'], ['dyed', 'my'], ['dyed', 'never'], ['dyed', 'hair'], ['dyed', 'ive'], ['dyed', 'but'], ['my', 'hair'], ['my', 'dyed'], ['my', 'never'], ['my', 'ive'], ['hair', 'my'], ['hair', 'dyed'], ['hair', 'never']]\n"
     ]
    }
   ],
   "source": [
    "print(word_lists[-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92603186-5a75-44c7-a22e-d9578041ea57",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## One-hot encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6cd1aa7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# Creating the dictionary for the unique words\n",
    "unique_word_dict = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    unique_word_dict.update({\n",
    "        word: i\n",
    "    })\n",
    "    \n",
    "print(unique_word_dict.get('a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc2bc7e6-7eb8-40cd-a635-1c57adf8502e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the number of features (unique words)\n",
    "n_words = len(unique_word_dict)\n",
    "\n",
    "# Getting all the unique words \n",
    "words = list(unique_word_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1d3c32-dfc3-4da8-89e7-9bdfde918571",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69330a86-280a-43d6-8db2-13a506233aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(word_lists): 89394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "89394it [00:00, 381958.76it/s]\n"
     ]
    }
   ],
   "source": [
    "# word_lists1 = word_lists[:int(len(word_lists)/10000)]\n",
    "print('len(word_lists): ' + str(len(word_lists)))\n",
    "#print('len(word_lists1): ' + str(len(word_lists1)))\n",
    "\n",
    "\n",
    "# Creating the X and Y matrices using one hot encoding\n",
    "X = np.zeros((len(word_lists),n_words),dtype=bool)\n",
    "Y = np.zeros((len(word_lists),n_words),dtype=bool)\n",
    "\n",
    "X.shape\n",
    "\n",
    "for i, word_list in tqdm(enumerate(word_lists)):\n",
    "    # Getting the indices\n",
    "    main_word_index = unique_word_dict.get(word_list[0])      #First word in tupel\n",
    "    context_word_index = unique_word_dict.get(word_list[1])   #Second word in Tupel\n",
    "\n",
    "    # One hot encoding the main word\n",
    "    X[i, main_word_index] = 1\n",
    "\n",
    "    # One hot encoding the Y matrix words \n",
    "    Y[i, context_word_index] = 1\n",
    "\n",
    "\n",
    "#! Hier schmiert der Kernel bei der Iteration immer ab.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db8f3112-1a2c-485d-b14d-315b6fbe2392",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-23 13:07:36.987440: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((X, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f68249c-3f3b-469f-ac86-11ec8952d04e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "667c730d-cdf9-484c-8877-1b3467300b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset element_spec=(TensorSpec(shape=(3454,), dtype=tf.bool, name=None), TensorSpec(shape=(3454,), dtype=tf.bool, name=None))>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb02a0c-f7c6-4a67-b5c5-4fa75c5ab519",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2343e975-ba2e-4874-a953-1ec4242fd5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the size of the embedding (Often in practice, pre-trained word embeddings are used with typical word embedding dimensions being either 100, 200 or 300.)\n",
    "embed_size = 100\n",
    "\n",
    "# Defining the neural network\n",
    "inp = Input(shape=(X.shape[1],))\n",
    "x = Dense(units=embed_size, activation='linear')(inp)\n",
    "x = Dense(units=Y.shape[1], activation='softmax')(x)\n",
    "model = Model(inputs=inp, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c1becab9-acad-42a0-85c0-1f6ed28542e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d561402-c328-4b88-b27d-be5c26a805a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 3454) for input KerasTensor(type_spec=TensorSpec(shape=(None, 3454), dtype=tf.float32, name='input_4'), name='input_4', description=\"created by layer 'input_4'\"), but it was called on an input with incompatible shape (3454,).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/training.py\", line 859, in train_step\n        y_pred = self(x, training=True)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 228, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n\n    ValueError: Exception encountered when calling layer \"model_3\" (type Functional).\n    \n    Input 0 of layer \"dense_6\" is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (3454,)\n    \n    Call arguments received:\n      • inputs=tf.Tensor(shape=(3454,), dtype=bool)\n      • training=True\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Optimizing the network weights\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py:1147\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m   1146\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1147\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[1;32m   1148\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/training.py\", line 859, in train_step\n        y_pred = self(x, training=True)\n    File \"/opt/conda/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/conda/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 228, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n\n    ValueError: Exception encountered when calling layer \"model_3\" (type Functional).\n    \n    Input 0 of layer \"dense_6\" is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (3454,)\n    \n    Call arguments received:\n      • inputs=tf.Tensor(shape=(3454,), dtype=bool)\n      • training=True\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "# Optimizing the network weights\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    batch_size=256,\n",
    "    epochs=50\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a10f29-e521-465d-b9d0-f08587754b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ac9b11-e35d-4fd5-abe9-40288bc0b0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining the weights from the neural network. \n",
    "# These are the so called word embeddings\n",
    "\n",
    "# The input layer \n",
    "weights = model.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a692f41-fbdf-419d-950b-f4abcc41955c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary to store the embeddings in. The key is a unique word and \n",
    "# the value is the numeric vector\n",
    "embedding_dict = {}\n",
    "for word in words: \n",
    "    embedding_dict.update({\n",
    "        word: np.asarray(weights[unique_word_dict.get(word)])\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0002f69-da90-4915-9111-dcea0af7c1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for word in list(unique_word_dict.keys()):\n",
    "  coord = embedding_dict.get(word)\n",
    "  plt.scatter(coord[0], coord[1])\n",
    "  plt.annotate(word, (coord[0], coord[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6079f9c6-643f-40d2-8c2b-22ad533568a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metadata into tsv file\n",
    "pd.DataFrame(embedding_dict.keys()).to_csv(\"model_dir/metadata.tsv\", sep = '\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3726a8a-e656-4bc8-824b-2ae2badec609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings_vectors into tsv file\n",
    "pd.DataFrame(embedding_dict.values()).to_csv(\"model_dir/vectors_2.tsv\", sep = '\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7e7d93-d42d-49e1-b2e7-77a4e680ff40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
