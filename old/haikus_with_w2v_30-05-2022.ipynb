{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cfb0df2-6691-42f0-a537-06a1ffbfd9a7",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc7d1ec-28ab-4353-9de5-9d0102b0d061",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "203148e5-af60-4988-b678-bda538d6249b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "from tqdm import tqdm\n",
    "import gensim\n",
    "\n",
    "# Deep learning: \n",
    "from keras.models import Input, Model\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4acdb5f-346c-45c3-afcb-b8af2ec10aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d02a6e-d7e4-49bb-85a8-db2d70189bb6",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c39059e-77d5-42d6-bdce-766e49e9f5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datensatz einlesen\n",
    "df = pd.read_csv('data/out.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbd8388-e4bc-4b0b-b6f2-c8ab37360f92",
   "metadata": {},
   "source": [
    "## Transform dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f19816b9-d18a-4c52-bda3-7aef12cb0867",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataframe mit 3 Spalten. Werden so gejoint, dass ein neues Dataframe mit ein Haiku pro Zeile erstellt wird\n",
    "df = df[['0', '1', '2']].agg(lambda x: ' '.join(x.values), axis=1)\n",
    "# Dataframe to list [[]] -> []\n",
    "haikus = df.values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2726013-e79e-4d25-b5da-a0fc4e2c45f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['last red in the sky a small girls moon face rises over the counter', 'christmas services a cellular phone rings out handels messiah', 'passover darkness  before the buds burst open a childs eyes in death', 'last night of summer the bright full moon of last night hidden by a cloud', 'midnight and full moon my neighbour asks to borrow the vacum cleaner', 'yellow walnut leaves slowly appear on the lawn early morning light', 'after its first flight the young gerfalcons talons tighter on my glove', 'sultry afternoon only the mailbox shadow crosses the dirt road', 'long journey back home  a forgotten bale of hay slowly rots away', 'autumn mist obscures the island in the distance she cleans her glasses']\n",
      "number of haikus: 93390\n"
     ]
    }
   ],
   "source": [
    "# alle Haikus in Array\n",
    "print(haikus[:10])\n",
    "number_of_haikus = len(haikus)\n",
    "print('number of haikus: ' + str(number_of_haikus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94a0b771-a466-4239-95a2-db1b428b1c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# die Haikus cleanen und selber auch noch mal als Wort-Arrays in gro√üen Array\n",
    "\n",
    "haikus = np.array(haikus)\n",
    "\n",
    "def clean_and_split(sentence):\n",
    "    return re.sub('[.,_]', '', sentence).split(' ')\n",
    "\n",
    "haikus = list(map(lambda x: clean_and_split(x), haikus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bac24142-4e2f-43fd-aeea-6c8d3cf31769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['last', 'red', 'in', 'the', 'sky', 'a', 'small', 'girls', 'moon', 'face', 'rises', 'over', 'the', 'counter'], ['christmas', 'services', 'a', 'cellular', 'phone', 'rings', 'out', 'handels', 'messiah'], ['passover', 'darkness', '', 'before', 'the', 'buds', 'burst', 'open', 'a', 'childs', 'eyes', 'in', 'death'], ['last', 'night', 'of', 'summer', 'the', 'bright', 'full', 'moon', 'of', 'last', 'night', 'hidden', 'by', 'a', 'cloud'], ['midnight', 'and', 'full', 'moon', 'my', 'neighbour', 'asks', 'to', 'borrow', 'the', 'vacum', 'cleaner'], ['yellow', 'walnut', 'leaves', 'slowly', 'appear', 'on', 'the', 'lawn', 'early', 'morning', 'light'], ['after', 'its', 'first', 'flight', 'the', 'young', 'gerfalcons', 'talons', 'tighter', 'on', 'my', 'glove'], ['sultry', 'afternoon', 'only', 'the', 'mailbox', 'shadow', 'crosses', 'the', 'dirt', 'road'], ['long', 'journey', 'back', 'home', '', 'a', 'forgotten', 'bale', 'of', 'hay', 'slowly', 'rots', 'away'], ['autumn', 'mist', 'obscures', 'the', 'island', 'in', 'the', 'distance', 'she', 'cleans', 'her', 'glasses']]\n"
     ]
    }
   ],
   "source": [
    "print(haikus[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "916379c1-2f07-4d1c-ac2f-c80e4f9207b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' harry potter chamber of secrets by j', ' k', ' rowling this book  in harry potter series original scannedocr friday april   v', ' edit where needed change version number by ', ' c h p t e o n e worst birthday not for first time an argument had broken out over breakfast at number four privet drive', ' mr', ' vernon dursley had been woken in early hours of morning by loud hooting noise from his nephew harrys room', ' third time this week he roared across table', ' if you cant control that owl itll have to go harry tried yet again to explain', ' shes bored he said']\n"
     ]
    }
   ],
   "source": [
    "def clean_text(\n",
    "    string: str, \n",
    "    punctuations=r'''!()-[]{};:'\"\\,<>/?@#$%^&*_~=`''',\n",
    "    stop_words=['the', 'a', 'and', 'is', 'be', 'will']) -> str:\n",
    "    \"\"\"\n",
    "    A method to clean text \n",
    "    \"\"\"\n",
    "    # Cleaning the urls\n",
    "    string = re.sub(r'https?://\\S+|www\\.\\S+', '', string)\n",
    "\n",
    "    # Cleaning the html elements\n",
    "    string = re.sub(r'<.*?>', '', string)\n",
    "\n",
    "    # Removing the punctuations\n",
    "    for x in string.lower(): \n",
    "        if x in punctuations: \n",
    "            string = string.replace(x, \"\") \n",
    "\n",
    "    # Converting the text to lower\n",
    "    string = string.lower()\n",
    "\n",
    "    # Removing stop words\n",
    "    string = ' '.join([word for word in string.split() if word not in stop_words])\n",
    "\n",
    "    # Cleaning the whitespaces\n",
    "    string = re.sub(r'\\s+', ' ', string).strip()\n",
    "    \n",
    "    # Removing numbers#\n",
    "    string = re.sub(r'\\d', '', string)\n",
    "\n",
    "    return string  \n",
    "\n",
    "# Datensatz einlesen\n",
    "path_to_file = tf.keras.utils.get_file('Harry%20Potter%202%20-%20Chamber%20of%20Secrets.txt',\n",
    "                                       'http://www.glozman.com/TextPages/Harry%20Potter%202%20-%20Chamber%20of%20Secrets.txt')\n",
    "\n",
    "# Read, then decode for py2 compat.\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "text = clean_text(text).split('.')\n",
    "#print(text[:10])\n",
    "\n",
    "hp_array = list(map(lambda x: clean_and_split(x), text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb02a0c-f7c6-4a67-b5c5-4fa75c5ab519",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2343e975-ba2e-4874-a953-1ec4242fd5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "model = gensim.models.Word2Vec(hp_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d491045-c758-4929-aaf2-4cf375433183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec<vocab=1943, vector_size=100, alpha=0.025>\n"
     ]
    }
   ],
   "source": [
    "# summarize the loaded model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd1d78a9-21c3-4ce8-b2fa-3e8d6886af63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector for 'girl':\n",
      "[-0.0489248   0.18681888  0.03319269  0.03603826  0.14984688 -0.1654673\n",
      "  0.15224434  0.3821418  -0.14198801 -0.05851124 -0.09174149 -0.2001487\n",
      "  0.01571479  0.0088606   0.20546536 -0.09817459  0.04179309 -0.06742151\n",
      "  0.00516966 -0.29627094  0.20089614  0.00789949  0.07393959 -0.14401254\n",
      "  0.00651163  0.05856685 -0.06709082 -0.19387507 -0.11185332  0.02075415\n",
      "  0.16296817  0.07231785 -0.00572357 -0.18847522 -0.04731258  0.18734443\n",
      " -0.02862754 -0.04059602 -0.030715   -0.27051035  0.10323879 -0.04915797\n",
      " -0.05320074  0.01392652  0.2391323  -0.07508141 -0.1710209   0.00318667\n",
      "  0.18462081  0.0140131   0.0197587  -0.11320188 -0.09901898 -0.04548169\n",
      "  0.02633374  0.04613921  0.0774593   0.1236912  -0.07847422  0.0735085\n",
      "  0.01917881  0.1024448   0.05188182 -0.00429058 -0.25333625  0.19324161\n",
      "  0.09037547  0.18758845 -0.16218056  0.15377995 -0.07535589  0.0314659\n",
      "  0.26763743 -0.08472732  0.20143652 -0.00042409 -0.0207449  -0.00779402\n",
      " -0.22393502  0.03520436 -0.07960196 -0.07678747 -0.09237269  0.25098005\n",
      " -0.03566802 -0.05341139  0.17343734  0.03758781  0.21818067 -0.05266856\n",
      "  0.22678629  0.10604841 -0.00718149  0.03589687  0.22429936  0.23287974\n",
      "  0.01957477 -0.1350586   0.01428589 -0.04383687]\n"
     ]
    }
   ],
   "source": [
    "# access vector for one word\n",
    "print('vector for \\'girl\\':')\n",
    "print(model.wv['girl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5fd3d026-5e46-405a-ae8e-ebab45ee2075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10 words most similar to 'girl':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('very', 0.999654233455658),\n",
       " ('toward', 0.9996378421783447),\n",
       " ('came', 0.9996365308761597),\n",
       " ('next', 0.9996352195739746),\n",
       " ('long', 0.9996311068534851),\n",
       " ('still', 0.9996151924133301),\n",
       " ('which', 0.999612033367157),\n",
       " ('few', 0.9996045231819153),\n",
       " ('again', 0.9996023178100586),\n",
       " ('first', 0.9996015429496765)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('top 10 words most similar to \\'girl\\':')\n",
    "model.wv.most_similar('car', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8ef099b-640b-4e25-99a0-6468adce2efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity between 'go' and 'walk' (regarding the haikus):\n",
      "0.99578285\n",
      "\n",
      "similarity between 'go' and 'laugh' (regarding the haikus):\n",
      "0.9971985\n",
      "\n",
      "similarity between 'go' and 'go':\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# similarity between two words\n",
    "print('similarity between \\'go\\' and \\'walk\\' (regarding the haikus):')\n",
    "print(model.wv.similarity(w1='go', w2='walk'))\n",
    "print()\n",
    "\n",
    "print('similarity between \\'go\\' and \\'laugh\\' (regarding the haikus):')\n",
    "print(model.wv.similarity(w1='go', w2='laugh'))\n",
    "print()\n",
    "\n",
    "print('similarity between \\'go\\' and \\'go\\':')\n",
    "print(model.wv.similarity(w1='go', w2='go'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef287aca-158d-47aa-a2ea-97b801155496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "#model.save('w2v_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "108a5499-de48-4826-9fda-37a5cd5d8f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "#new_model = Word2Vec.load('w2v_model.bin')\n",
    "#print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb90c737-8d1a-4706-b013-f2a84b5950b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectors:\n",
      "[[-0.13968319  0.5039783   0.08849982  0.08344058  0.37877145 -0.44779658\n",
      "   0.3779186   1.0413792  -0.3710803  -0.18029764 -0.18503423 -0.52688724\n",
      "  -0.01334676  0.0043259   0.50906676 -0.28334302  0.11979121 -0.14377664\n",
      "   0.05835876 -0.7649864   0.50436354  0.01947019  0.20050618 -0.3582738\n",
      "   0.0431937   0.16984197 -0.1581541  -0.51988554 -0.27227524  0.01201792\n",
      "   0.37299243  0.20391868  0.0031895  -0.50887203 -0.08475018  0.46862692\n",
      "  -0.04682276 -0.13558753 -0.07548592 -0.7260794   0.26088592 -0.068873\n",
      "  -0.16426778  0.0215367   0.6039142  -0.15868577 -0.47599307  0.05856868\n",
      "   0.5057014   0.03344871  0.05360534 -0.29869542 -0.25509396 -0.06994253\n",
      "   0.09403911  0.10323661  0.18496366  0.29347643 -0.19522241  0.19901723\n",
      "  -0.00849691  0.27679744  0.12782975 -0.0200706  -0.660942    0.5165119\n",
      "   0.22386695  0.4763842  -0.46559688  0.4260287  -0.18487327  0.11791668\n",
      "   0.71495295 -0.20919794  0.51777965 -0.01633553 -0.04108058 -0.04874487\n",
      "  -0.5648129   0.09661488 -0.22110282 -0.22063653 -0.259746    0.66811067\n",
      "  -0.12317441 -0.13303162  0.4981881   0.10585317  0.53708124 -0.11116751\n",
      "   0.6163644   0.2998919  -0.03123615  0.1162027   0.6254295   0.58907807\n",
      "  -0.00408938 -0.3579302   0.01091703 -0.09878152]\n",
      " [-0.16419624  0.53554374  0.09636913  0.10414223  0.4304256  -0.4668813\n",
      "   0.42647153  1.1026201  -0.3864827  -0.16718744 -0.23562951 -0.5718895\n",
      "   0.015544    0.00930451  0.5537597  -0.26442903  0.13999568 -0.14449888\n",
      "   0.04696768 -0.80107576  0.5878699   0.01956782  0.20715022 -0.40453705\n",
      "   0.0456119   0.19061364 -0.17192654 -0.5628433  -0.2790122   0.05118104\n",
      "   0.4415637   0.20500731 -0.01535968 -0.52774906 -0.0853884   0.50063574\n",
      "  -0.05568324 -0.12530576 -0.07460772 -0.7458148   0.30315727 -0.11363217\n",
      "  -0.1674692   0.0342802   0.63684815 -0.17357598 -0.46335834 -0.00280874\n",
      "   0.5269428   0.05035027  0.06834909 -0.33374316 -0.28343692 -0.08398369\n",
      "   0.08428877  0.15217689  0.20988744  0.33313286 -0.21723606  0.2339447\n",
      "   0.00620811  0.30047283  0.13652676 -0.00375094 -0.6880405   0.5598858\n",
      "   0.2544684   0.48424503 -0.4392384   0.46397865 -0.2253457   0.07601873\n",
      "   0.7803786  -0.2156416   0.52790844 -0.01760776 -0.0811474  -0.05725699\n",
      "  -0.59029466  0.09217648 -0.2562339  -0.23001167 -0.28155613  0.68192285\n",
      "  -0.09249914 -0.14487998  0.49706367  0.09239788  0.5848547  -0.16388503\n",
      "   0.64580995  0.32452247 -0.02157038  0.09311043  0.63221544  0.63234174\n",
      "   0.03368304 -0.3647898   0.00324441 -0.10810298]]\n",
      "\n",
      "labels:\n",
      "['' 'to' 'of' 'harry' 'he' 'was' 'said' 'his' 'in' 'it']\n"
     ]
    }
   ],
   "source": [
    "# extract the words & their vectors, as numpy arrays\n",
    "vectors = np.asarray(model.wv.vectors)\n",
    "labels = np.asarray(model.wv.index_to_key)  # fixed-width numpy strings\n",
    "\n",
    "print('vectors:')\n",
    "print(vectors[:2])\n",
    "print()\n",
    "print('labels:')\n",
    "print(labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa22bfde-7705-43b1-a949-9610f087d26a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1943"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fde1864f-9255-4257-b256-3393ca0d312d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1943"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf6e4f2-1ec2-4371-8158-7fb7879527d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://projector.tensorflow.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a668015a-c472-4633-b2f2-2b134fa7c817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metadata (labels) into tsv file\n",
    "pd.DataFrame(labels).to_csv(\"model_dir/metadata.tsv\", sep = '\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5e66453-e7ed-49b0-99c3-7cbb96887765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save vectors into tsv file\n",
    "pd.DataFrame(vectors).to_csv(\"model_dir/vectors.tsv\", sep = '\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
