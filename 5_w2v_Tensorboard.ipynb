{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cfb0df2-6691-42f0-a537-06a1ffbfd9a7",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc7d1ec-28ab-4353-9de5-9d0102b0d061",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "203148e5-af60-4988-b678-bda538d6249b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "from tqdm import tqdm\n",
    "from scipy import sparse\n",
    "\n",
    "# Deep learning: \n",
    "from keras.models import Input, Model\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29072d0f-c7ac-47f2-83f5-f809edd42372",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activate tensorboard extension\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "#Import necessary libs\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorboard.plugins import projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4acdb5f-346c-45c3-afcb-b8af2ec10aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-30 10:05:44.851727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-30 10:05:44.959195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-30 10:05:44.959790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d02a6e-d7e4-49bb-85a8-db2d70189bb6",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c39059e-77d5-42d6-bdce-766e49e9f5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datensatz einlesen\n",
    "df = pd.read_csv('data/out.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbd8388-e4bc-4b0b-b6f2-c8ab37360f92",
   "metadata": {},
   "source": [
    "## Transform dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f19816b9-d18a-4c52-bda3-7aef12cb0867",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataframe mit 3 Spalten. Werden so gejoint, dass ein neues Dataframe mit ein Haiku pro Zeile erstellt wird\n",
    "df = df[['0', '1', '2']].agg(lambda x: ' '.join(x.values), axis=1)\n",
    "# Dataframe to list [[]] -> []\n",
    "haikus = df.values.tolist()\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Erst mal nur mit 1000 Haikus!\n",
    "haikus = haikus[:5000]\n",
    "# ---------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2726013-e79e-4d25-b5da-a0fc4e2c45f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['its finals week so  im gonna be less active than normal love yall', 'money attract hoes  but i dont want hoes that shit getting so boring', 'without coffee i  dont think i could get out of bed in the mornings', 'you bitches are wild  believe it or not your friend can have other friends', 'i dont really care  if you cry on the real you shouldve never lied', 'i need a logo  i hope the logo maker does something i like', 'i dont understand  how people go to the gym and dont sweat at all', 'does anyone know  of a good accessible rooftop in munich', 'thinking of going  to see bae but then rain and tests are on my case', 'let yourself be drawn  by the stronger pull of that which you truly love']\n",
      "number of haikus: 5000\n"
     ]
    }
   ],
   "source": [
    "# alle Haikus in Array\n",
    "print(haikus[-10:])\n",
    "number_of_haikus = len(haikus)\n",
    "print('number of haikus: ' + str(number_of_haikus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34535e9-892b-41b4-b2cb-a89c60fdfcfb",
   "metadata": {},
   "source": [
    "## Clean text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05d688c4-b56f-46fc-a88c-862b2ef63d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "huge_string = ' SATZENDE SATZANFANG '.join(haikus)\n",
    "huge_string_cleaned = re.sub('[.,_]', '', huge_string)\n",
    "\n",
    "# TODO remove stopwords!\n",
    "\n",
    "huge_list = huge_string_cleaned.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f7bbaa9-0dd9-4e40-ba91-a19200826abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'SATZANFANG', 'SATZENDE', 'a', 'aaron', 'abandon', 'abandoned', 'abandoning', 'abbey', 'abducted', 'ability', 'able', 'aboard', 'aboating', 'abolition', 'abort', 'abound', 'about', 'above', 'abroad']\n",
      "7942\n"
     ]
    }
   ],
   "source": [
    "#Get vocabulary\n",
    "vocab = sorted(set(huge_list))\n",
    "print(vocab[:20])\n",
    "vocab_size = len(vocab)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ab98fdc-692c-4c25-b965-ebe4831ee1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.array(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc70bbf-d679-4605-9da5-a35f29f57a1b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Create context windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29c70db9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Defining the window for context\n",
    "window = 3\n",
    "\n",
    "word_lists = []\n",
    "\n",
    "# Creating a context dictionary\n",
    "for i, word in enumerate(huge_list):\n",
    "    for w in range(window):\n",
    "        # Getting the context that is ahead by *window* words\n",
    "        if i + 1 + w < len(huge_list): \n",
    "            word_lists.append([word] + [huge_list[(i + 1 + w)]])\n",
    "        # Getting the context that is behind by *window* words    \n",
    "        if i - w - 1 >= 0:\n",
    "            word_lists.append([word] + [huge_list[(i - w - 1)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcd757b2-c8b1-41ea-901a-967cfbfb334e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['case', 'SATZANFANG'], ['case', 'on'], ['case', 'let'], ['case', 'are'], ['SATZENDE', 'SATZANFANG'], ['SATZENDE', 'case'], ['SATZENDE', 'let'], ['SATZENDE', 'my'], ['SATZENDE', 'yourself'], ['SATZENDE', 'on'], ['SATZANFANG', 'let'], ['SATZANFANG', 'SATZENDE'], ['SATZANFANG', 'yourself'], ['SATZANFANG', 'case'], ['SATZANFANG', 'be'], ['SATZANFANG', 'my'], ['let', 'yourself'], ['let', 'SATZANFANG'], ['let', 'be'], ['let', 'SATZENDE'], ['let', 'drawn'], ['let', 'case'], ['yourself', 'be'], ['yourself', 'let'], ['yourself', 'drawn'], ['yourself', 'SATZANFANG'], ['yourself', ''], ['yourself', 'SATZENDE'], ['be', 'drawn'], ['be', 'yourself'], ['be', ''], ['be', 'let'], ['be', 'by'], ['be', 'SATZANFANG'], ['drawn', ''], ['drawn', 'be'], ['drawn', 'by'], ['drawn', 'yourself'], ['drawn', 'the'], ['drawn', 'let'], ['', 'by'], ['', 'drawn'], ['', 'the'], ['', 'be'], ['', 'stronger'], ['', 'yourself'], ['by', 'the'], ['by', ''], ['by', 'stronger'], ['by', 'drawn'], ['by', 'pull'], ['by', 'be'], ['the', 'stronger'], ['the', 'by'], ['the', 'pull'], ['the', ''], ['the', 'of'], ['the', 'drawn'], ['stronger', 'pull'], ['stronger', 'the'], ['stronger', 'of'], ['stronger', 'by'], ['stronger', 'that'], ['stronger', ''], ['pull', 'of'], ['pull', 'stronger'], ['pull', 'that'], ['pull', 'the'], ['pull', 'which'], ['pull', 'by'], ['of', 'that'], ['of', 'pull'], ['of', 'which'], ['of', 'stronger'], ['of', 'you'], ['of', 'the'], ['that', 'which'], ['that', 'of'], ['that', 'you'], ['that', 'pull'], ['that', 'truly'], ['that', 'stronger'], ['which', 'you'], ['which', 'that'], ['which', 'truly'], ['which', 'of'], ['which', 'love'], ['which', 'pull'], ['you', 'truly'], ['you', 'which'], ['you', 'love'], ['you', 'that'], ['you', 'of'], ['truly', 'love'], ['truly', 'you'], ['truly', 'which'], ['truly', 'that'], ['love', 'truly'], ['love', 'you'], ['love', 'which']]\n"
     ]
    }
   ],
   "source": [
    "print(word_lists[-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92603186-5a75-44c7-a22e-d9578041ea57",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## One-hot encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6cd1aa7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# Creating the dictionary for the unique words\n",
    "unique_word_dict = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    unique_word_dict.update({\n",
    "        word: i\n",
    "    })\n",
    "    \n",
    "print(unique_word_dict.get('a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc2bc7e6-7eb8-40cd-a635-1c57adf8502e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the number of features (unique words)\n",
    "n_words = len(unique_word_dict)\n",
    "\n",
    "# Getting all the unique words \n",
    "words = list(unique_word_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69330a86-280a-43d6-8db2-13a506233aab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(word_lists): 469434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "469434it [00:02, 197887.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# word_lists1 = word_lists[:int(len(word_lists)/10000)]\n",
    "print('len(word_lists): ' + str(len(word_lists)))\n",
    "#print('len(word_lists1): ' + str(len(word_lists1)))\n",
    "\n",
    "\n",
    "# Creating the X and Y matrices using one hot encoding\n",
    "X = np.zeros((len(word_lists), n_words), dtype=bool)\n",
    "Y = np.zeros((len(word_lists), n_words), dtype=bool)\n",
    "\n",
    "for i, word_list in tqdm(enumerate(word_lists)):\n",
    "    # Getting the indices\n",
    "    main_word_index = unique_word_dict.get(word_list[0])      #First word in tupel\n",
    "    context_word_index = unique_word_dict.get(word_list[1])   #Second word in Tupel\n",
    "\n",
    "    # One hot encoding the main word\n",
    "    X[i, main_word_index] = 1\n",
    "\n",
    "    # One hot encoding the Y matrix words \n",
    "    Y[i, context_word_index] = 1\n",
    "\n",
    "\n",
    "#! Hier schmiert der Kernel bei der Iteration immer ab.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8f3112-1a2c-485d-b14d-315b6fbe2392",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((X, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667c730d-cdf9-484c-8877-1b3467300b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb02a0c-f7c6-4a67-b5c5-4fa75c5ab519",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2343e975-ba2e-4874-a953-1ec4242fd5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the size of the embedding (Often in practice, pre-trained word embeddings are used with typical word embedding dimensions being either 100, 200 or 300.)\n",
    "embed_size = 100\n",
    "\n",
    "# Defining the neural network\n",
    "inp = Input(shape=(X.shape[1],))\n",
    "x = Dense(units=embed_size, activation='linear')(inp)\n",
    "x = Dense(units=Y.shape[1], activation='softmax')(x)\n",
    "model = Model(inputs=inp, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1becab9-acad-42a0-85c0-1f6ed28542e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c1e397-61a0-4916-bb1e-aad6fc542134",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_in_batches():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d561402-c328-4b88-b27d-be5c26a805a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optimizing the network weights\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    batch_size=256,\n",
    "    epochs=50\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a10f29-e521-465d-b9d0-f08587754b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ac9b11-e35d-4fd5-abe9-40288bc0b0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining the weights from the neural network. \n",
    "# These are the so called word embeddings\n",
    "\n",
    "# The input layer \n",
    "weights = model.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a692f41-fbdf-419d-950b-f4abcc41955c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary to store the embeddings in. The key is a unique word and \n",
    "# the value is the numeric vector\n",
    "embedding_dict = {}\n",
    "for word in words: \n",
    "    embedding_dict.update({\n",
    "        word: np.asarray(weights[unique_word_dict.get(word)])\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0002f69-da90-4915-9111-dcea0af7c1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for word in list(unique_word_dict.keys()):\n",
    "    coord = embedding_dict.get(word)\n",
    "    plt.scatter(coord[0], coord[1])\n",
    "    plt.annotate(word, (coord[0], coord[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6079f9c6-643f-40d2-8c2b-22ad533568a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metadata into tsv file\n",
    "pd.DataFrame(embedding_dict.keys()).to_csv(\"model_dir/metadata.tsv\", sep = '\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3726a8a-e656-4bc8-824b-2ae2badec609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings_vectors into tsv file\n",
    "pd.DataFrame(embedding_dict.values()).to_csv(\"model_dir/vectors_2.tsv\", sep = '\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
