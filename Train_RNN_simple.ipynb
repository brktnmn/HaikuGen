{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cfb0df2-6691-42f0-a537-06a1ffbfd9a7",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc7d1ec-28ab-4353-9de5-9d0102b0d061",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "203148e5-af60-4988-b678-bda538d6249b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4acdb5f-346c-45c3-afcb-b8af2ec10aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 14:02:56.307294: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-02 14:02:56.354695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-02 14:02:56.355205: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d02a6e-d7e4-49bb-85a8-db2d70189bb6",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c39059e-77d5-42d6-bdce-766e49e9f5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datensatz einlesen\n",
    "df = pd.read_csv('data/out.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbd8388-e4bc-4b0b-b6f2-c8ab37360f92",
   "metadata": {},
   "source": [
    "## Transform dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f19816b9-d18a-4c52-bda3-7aef12cb0867",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataframe mit 3 Spalten. Werden so gejoint, dass ein neues Dataframe mit ein Haiku pro Zeile erstellt wird\n",
    "df = df[['0', '1', '2']].agg(lambda x: '\\n'.join(x.values), axis=1)\n",
    "# Dataframe to list [[]] -> []\n",
    "array_of_poems = df.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2726013-e79e-4d25-b5da-a0fc4e2c45f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_poems an Haikus mit Semicolon zwshceneinander zusammenfuegen\n",
    "num_poems = 15000\n",
    "\n",
    "text = ';'.join(array_of_poems[:num_poems])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34535e9-892b-41b4-b2cb-a89c60fdfcfb",
   "metadata": {},
   "source": [
    "## Get unique chars in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05d688c4-b56f-46fc-a88c-862b2ef63d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 unique chars\n",
      "['\\n', ' ', ';', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# Anzahl der unterschielichen characters im gesamt datensatz herausfinden\n",
    "vocab = sorted(set(text))\n",
    "vocab_size = len(vocab)\n",
    "print(f'{vocab_size} unique chars')\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc70bbf-d679-4605-9da5-a35f29f57a1b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Create training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29c70db9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chars: 29\n",
      "Number of sequences: 58171\n"
     ]
    }
   ],
   "source": [
    "print(\"Total chars:\", vocab_size)\n",
    "# Dictionary erstellen. Jeder character wird mit einer Zahl nummeriert\n",
    "char_indices = {c: i for i, c in enumerate(vocab)}\n",
    "indices_char = {i: c for i, c in enumerate(vocab)}\n",
    "\n",
    "# cut the text in semi-redundant sequences of seq_len characters\n",
    "seq_len = 100\n",
    "step = 17\n",
    "# Input String\n",
    "sequences = []\n",
    "#Output character\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - seq_len, step):\n",
    "    sequences.append(text[i : i + seq_len])\n",
    "    next_chars.append(text[i + seq_len])\n",
    "print(\"Number of sequences:\", len(sequences))\n",
    "\n",
    "# Input String onehot encoded\n",
    "x = np.zeros((len(sequences), seq_len, vocab_size), dtype=int)\n",
    "# Output char onehot encoded\n",
    "y = np.zeros((len(sequences), vocab_size), dtype=int)\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c82072d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6cd1aa7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e5b39dc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 14:03:03.589181: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-02 14:03:03.590433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-02 14:03:03.591066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-02 14:03:03.591472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-02 14:03:04.591105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-02 14:03:04.591761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-02 14:03:04.592220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-02 14:03:04.592695: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6114 MB memory:  -> device: 0, name: GRID V100S-8Q, pci bus id: 0000:06:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        # input_shape=sequenz laenge, vocab_size\n",
    "        # return sequences true -> input-shape = output-shape \n",
    "        # shape-input (NONE, seq_len, vocab_size)\n",
    "        layers.LSTM(128, input_shape=(x.shape[1], x.shape[2]), return_sequences=True),\n",
    "        layers.Dropout(0.2),\n",
    "        # shape-input (NONE, seq_len, vocab_size)\n",
    "        layers.LSTM(128, return_sequences=True),\n",
    "        layers.Dropout(0.2),\n",
    "        # shape-input (NONE, seq_len, vocab_size)\n",
    "        layers.LSTM(64),\n",
    "        # shape-input (NONE, vocab_size)\n",
    "        layers.Dense(vocab_size, activation=\"softmax\"),\n",
    "        # bsp out [0.3, 0.2, 0.1, 0.4]\n",
    "    ]\n",
    ")\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60703971",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 100, 128)          80896     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 100, 128)          0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100, 128)          131584    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 100, 128)          0         \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 29)                1885      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 263,773\n",
      "Trainable params: 263,773\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b985dc-82a4-41ac-a107-bf41c1137a00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4323d5ed",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Standartfunktion Probability array to onehot to integerencoded char \n",
    "# [0.3, 0.2, 0.1, 0.4] -> [0, 0, 0, 1] -> return 4 (stelle, an der 1)\n",
    "def sample(prob, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    prob = np.asarray(prob).astype(\"float64\")\n",
    "    prob = np.log(prob) / temperature\n",
    "    exp_prob = np.exp(prob)\n",
    "    prob = exp_prob / np.sum(exp_prob)\n",
    "    probas = np.random.multinomial(1, prob)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1e8557",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "EPOCH:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 14:03:05.543949: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1349567200 exceeds 10% of free system memory.\n",
      "2022-05-02 14:03:06.957561: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1349567200 exceeds 10% of free system memory.\n",
      "2022-05-02 14:03:12.516088: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "455/455 [==============================] - 19s 29ms/step - loss: 2.6825\n",
      "\n",
      " wyhler ce dilt lewyore weenibe folib cat ke deere phe lafe;\n",
      "----------------------------------------\n",
      "doad o gang hhere me mad on the pe ashhaug snertacle the;\n",
      "----------------------------------------\n",
      "i g\n",
      "uke in\n",
      "sint nhe tt\n",
      " le\n",
      "toa if fu sfaanvp fagarly ghey goled it;\n",
      "----------------------------------------\n",
      "torpa\n",
      ";\n",
      "----------------------------------------\n",
      "mon he an\n",
      "\n",
      "\n",
      "EPOCH:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 14:03:39.865225: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1349567200 exceeds 10% of free system memory.\n",
      "2022-05-02 14:03:41.264007: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1349567200 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "455/455 [==============================] - 13s 29ms/step - loss: 2.2763\n",
      "\n",
      "\n",
      " thas to thaseltise yore\n",
      "thing\n",
      "i uls porreon wanester me;\n",
      "----------------------------------------\n",
      "tarefad waot op i heatted\n",
      "boll\n",
      "i  drorle;\n",
      "----------------------------------------\n",
      "shay he ron berorrerly\n",
      "cus warkice;\n",
      "----------------------------------------\n",
      "gave\n",
      " is oke peally will;\n",
      "----------------------------------------\n",
      "bet\n",
      "naniys seeself my nhat to in vur;\n",
      "----------------------------------------\n",
      "be\n",
      " \n",
      "\n",
      "\n",
      "EPOCH:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 14:04:07.000338: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1349567200 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "455/455 [==============================] - 13s 29ms/step - loss: 2.1112\n",
      "\n",
      " kleming\n",
      "goer\n",
      "all of di\n",
      "magborice\n",
      " you feo asbat and span wad it the west bigh yes as nottion wepa\n",
      " iclonsesangred as the frage;\n",
      "----------------------------------------\n",
      "beceels is shate anror\n",
      "theerls onlesch then peoke thes\n",
      " beach pirethiul \n",
      "\n",
      "\n",
      "EPOCH:3\n",
      "455/455 [==============================] - 13s 29ms/step - loss: 2.0032\n",
      "\n",
      "o to fraw in masten your that the\n",
      " ank gattle the sever iver at chuzt\n",
      " to ile im arrink;\n",
      "----------------------------------------\n",
      "have\n",
      "theme\n",
      " a gamp are faxt if whele that chall;\n",
      "----------------------------------------\n",
      "every in\n",
      "shatuare so im chafing can stirther\n",
      " right lonest pough\n",
      "\n",
      "\n",
      "EPOCH:4\n",
      "455/455 [==============================] - 13s 29ms/step - loss: 1.9220\n",
      "\n",
      "marie;\n",
      "----------------------------------------\n",
      "and noth\n",
      "rud to;\n",
      "----------------------------------------\n",
      "morid;\n",
      "----------------------------------------\n",
      "\n",
      "aint to bet ople\n",
      " as mat tuck u los belought of cicking\n",
      "yast the glate no thiivs;\n",
      "----------------------------------------\n",
      "sup dig will be um\n",
      " toy i cosples in lot mastlitet\n",
      " lought mying ow to dead gos tee tame\n",
      " \n",
      "\n",
      "\n",
      "EPOCH:5\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 1.8714\n",
      "\n",
      "m lost\n",
      " the mays ands pein ats\n",
      " and in toron with withing at ive\n",
      "of and to gews;\n",
      "----------------------------------------\n",
      "in on soshicgry\n",
      "i turting not jomny me sostly deel;\n",
      "----------------------------------------\n",
      "bees just des bads nothing carce neven\n",
      " to are word pan in coy;\n",
      "----------------------------------------\n",
      "i shou\n",
      "\n",
      "\n",
      "EPOCH:6\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 1.8264\n",
      "\n",
      "ke lack vou say who will;\n",
      "----------------------------------------\n",
      "ristoll in nex treed maken;\n",
      "----------------------------------------\n",
      "reawing geto sea as\n",
      "soun say the better;\n",
      "----------------------------------------\n",
      "some semion emaced\n",
      " know bart and to\n",
      " love like taking;\n",
      "----------------------------------------\n",
      "ive purae hey;\n",
      "----------------------------------------\n",
      "treads is to moaked\n",
      "pigar of of betper\n",
      "\n",
      "\n",
      "\n",
      "EPOCH:7\n",
      "455/455 [==============================] - 13s 29ms/step - loss: 1.7917\n",
      "\n",
      "o todery;\n",
      "----------------------------------------\n",
      "done your wether\n",
      "stilliop but alling\n",
      "start hardiel and coulde\n",
      "coming rerojh never\n",
      " this thing jeol defelts bes\n",
      "me fanides to god badere;\n",
      "----------------------------------------\n",
      "thonerdass this a justiliem;\n",
      "----------------------------------------\n",
      "that fulking;\n",
      "----------------------------------------\n",
      "og tracroad je\n",
      "\n",
      "\n",
      "EPOCH:8\n",
      "455/455 [==============================] - 13s 29ms/step - loss: 1.7671\n",
      "\n",
      "ing the\n",
      " gonna mit issing\n",
      " to my posten the ill like\n",
      " frease pleave the a\n",
      "oken this getting make\n",
      "grady think now;\n",
      "----------------------------------------\n",
      "im siching\n",
      " franing with me where\n",
      "frautely jancire;\n",
      "----------------------------------------\n",
      "i felce;\n",
      "----------------------------------------\n",
      "and of there my frop\n",
      " will f\n",
      "\n",
      "\n",
      "EPOCH:9\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 1.7460\n",
      "\n",
      " hates that buop\n",
      " know to fing is trander\n",
      " attasted a bambion\n",
      "i centugy whatel no\n",
      " dont enve my whileynt\n",
      " hope ary lol something eaut you\n",
      " to you upon through\n",
      "a puck him hered day it great\n",
      "in sew i pu\n",
      "\n",
      "\n",
      "EPOCH:10\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 1.7267\n",
      "\n",
      "owest but;\n",
      "----------------------------------------\n",
      "fool the bad;\n",
      "----------------------------------------\n",
      "to heve your ass sea\n",
      " off not end these your to\n",
      " are you life over you;\n",
      "----------------------------------------\n",
      "im cannoing hesore\n",
      "baby time joj for i catice suss want;\n",
      "----------------------------------------\n",
      "who about all lihs to fraed\n",
      " or is talking;\n",
      "----------------------------------------\n",
      "has set\n",
      "\n",
      "\n",
      "EPOCH:11\n",
      "455/455 [==============================] - 13s 29ms/step - loss: 1.7131\n",
      "\n",
      "d being im this schride\n",
      " peaping my mef see old;\n",
      "----------------------------------------\n",
      "peasonactly too more with;\n",
      "----------------------------------------\n",
      "presic a mable its see\n",
      "in ned unaocac on way mind the\n",
      "it rupher the gaclets pyod\n",
      " im the from you you you do a\n",
      " bath gonny gaa\n",
      "\n",
      "\n",
      "EPOCH:12\n",
      "455/455 [==============================] - 14s 30ms/step - loss: 1.6976\n",
      "\n",
      "\n",
      "and when people tell easted where;\n",
      "----------------------------------------\n",
      "noters live with tist\n",
      " selvil waits are like\n",
      " the for third in you me not\n",
      "snow of borepe these\n",
      " draves to like thoses\n",
      "if dring too feel scrove\n",
      " protings the movil;\n",
      "----------------------------------------\n",
      "ra\n",
      "\n",
      "\n",
      "EPOCH:13\n",
      "455/455 [==============================] - 13s 29ms/step - loss: 1.6832\n",
      "\n",
      "ies mits\n",
      " krock any someone sorry\n",
      " ne bemiactly beg\n",
      " likes about people;\n",
      "----------------------------------------\n",
      "thenses wasing the want\n",
      "because taken in a\n",
      " wrang out myself\n",
      "pummer on in my\n",
      " many yald and my\n",
      " just mories alone;\n",
      "----------------------------------------\n",
      "up dit that on\n",
      "\n",
      "\n",
      "EPOCH:14\n",
      "455/455 [==============================] - 13s 29ms/step - loss: 1.6715\n",
      "\n",
      " lots corsang up;\n",
      "----------------------------------------\n",
      "pettent never who\n",
      "anyone luts;\n",
      "----------------------------------------\n",
      "nighters veuling;\n",
      "----------------------------------------\n",
      "and body fuws does;\n",
      "----------------------------------------\n",
      "i bacrind for your mow\n",
      "want made that from when hawd\n",
      "my dont joll who bistave\n",
      "xosion on of\n",
      "happine is you retum;\n",
      "----------------------------------------\n",
      "my du\n",
      "\n",
      "\n",
      "EPOCH:15\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 1.6622\n",
      "\n",
      "v;\n",
      "----------------------------------------\n",
      "jarmy one as\n",
      " be again;\n",
      "----------------------------------------\n",
      "i next i dabnible\n",
      "when years takens damked mess\n",
      "your before in say\n",
      "you cactlize now strupt and with;\n",
      "----------------------------------------\n",
      "say black to frarcher everybody;\n",
      "----------------------------------------\n",
      "you minody fanching\n",
      "or he your noted\n",
      "do whe\n",
      "\n",
      "\n",
      "EPOCH:16\n",
      "455/455 [==============================] - 13s 29ms/step - loss: 1.6565\n",
      "\n",
      " frecking\n",
      " because worried on he phoose;\n",
      "----------------------------------------\n",
      "at pooped are;\n",
      "----------------------------------------\n",
      "i feel been be first\n",
      "not hate your all on a\n",
      " tranrey want ded i\n",
      " dincer of appother;\n",
      "----------------------------------------\n",
      "yex host times;\n",
      "----------------------------------------\n",
      "worusitad;\n",
      "----------------------------------------\n",
      "i getty\n",
      " her be serving play qeell yo\n",
      "\n",
      "\n",
      "EPOCH:17\n",
      "455/455 [==============================] - 13s 29ms/step - loss: 1.6517\n",
      "\n",
      "e only\n",
      "schuicactured i\n",
      "never keep has forgothes\n",
      " see qorry would the\n",
      "it wive have hard spob you\n",
      " bark picgle if why thos it\n",
      "theer and kind these if yeart\n",
      "win on i get so vippy\n",
      "way at about and glads\n",
      " \n",
      "\n",
      "\n",
      "EPOCH:18\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 1.6405\n",
      "\n",
      "always list guit bared\n",
      " the pan is youre i\n",
      " just do me bodee;\n",
      "----------------------------------------\n",
      "black i patal jool for\n",
      "this everyone hawk ver when\n",
      " scare make be someone\n",
      "none win urself for toust dont\n",
      " it is is newre to be use;\n",
      "----------------------------------------\n",
      "fuck and\n",
      "\n",
      "\n",
      "EPOCH:19\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 1.6358\n",
      "\n",
      "afine can;\n",
      "----------------------------------------\n",
      "sleep it a beanrets the growed;\n",
      "----------------------------------------\n",
      "it bad any can;\n",
      "----------------------------------------\n",
      "ig im there trinds;\n",
      "----------------------------------------\n",
      "stop fan i lately\n",
      " cith to hive it;\n",
      "----------------------------------------\n",
      "can first lyaang\n",
      "has all can letter the with\n",
      "the emt as havewu;\n",
      "----------------------------------------\n",
      "the s never have you\n",
      " heart\n",
      "\n",
      "\n",
      "EPOCH:20\n",
      "455/455 [==============================] - 13s 28ms/step - loss: 1.6251\n",
      "\n",
      "eady to go\n",
      "but u so have i just for\n",
      "ges ass out up has of stiph;\n",
      "----------------------------------------\n",
      "its at try i;\n",
      "----------------------------------------\n",
      "ill get came;\n",
      "----------------------------------------\n",
      "otal in;\n",
      "----------------------------------------\n",
      "got not trunry;\n",
      "----------------------------------------\n",
      "complet for there its\n",
      " ever good in someance\n",
      " drage everyone remo\n",
      "avalil or brissed;\n",
      "----------------------------------------\n",
      "hap\n",
      "\n",
      "\n",
      "EPOCH:21\n",
      "455/455 [==============================] - 13s 29ms/step - loss: 1.6233\n",
      "\n",
      "other come your come on\n",
      " please like where nothing\n",
      " awace looking my preck\n",
      "good clotul canna\n",
      " tafing worm this out\n",
      " you want everyone for\n",
      " smale or mum;\n",
      "----------------------------------------\n",
      "i exsdere worpion\n",
      "notion itfow is forget\n",
      "gladen \n",
      "\n",
      "\n",
      "EPOCH:22\n",
      "455/455 [==============================] - 13s 29ms/step - loss: 1.6114\n",
      "\n",
      "e not with to be;\n",
      "----------------------------------------\n",
      "time more ubssronger\n",
      " on when youd from hol you renday\n",
      " is that and from that the romry\n",
      " propest of the idly\n",
      "a less better up;\n",
      "----------------------------------------\n",
      "i replesses to be\n",
      " ats its crourt with eve\n",
      "easing off tha\n",
      "\n",
      "\n",
      "EPOCH:23\n",
      "455/455 [==============================] - 13s 29ms/step - loss: 1.6104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 25\n",
    "batch_size = 128\n",
    "\n",
    "input_data = x\n",
    "output_data = y\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print()\n",
    "    print()\n",
    "    print(f\"EPOCH:{epoch}\")\n",
    "    model.fit(input_data, output_data, batch_size=batch_size, epochs=1)\n",
    "\n",
    "    print()\n",
    "\n",
    "    generate_chars = 200\n",
    "    temperature = 1.0\n",
    "    start_index = random.randint(0, len(text) - seq_len - 1)\n",
    "    generated = \"\"\n",
    "\n",
    "    seed =  text[start_index : start_index + seq_len]\n",
    "    \n",
    "    #print('...Generating with seed: \"' + seed + '\"')\n",
    "\n",
    "    for i in range(generate_chars):\n",
    "        x_pred = np.zeros((1, len(seed), vocab_size))\n",
    "        for t, char in enumerate(seed):\n",
    "            x_pred[0, t, char_indices[char]] = 1\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        \n",
    "        next_index = sample(preds, temperature)\n",
    "        next_char = indices_char[next_index]\n",
    "        seed = seed[1:] + next_char\n",
    "        generated += next_char\n",
    "        \n",
    "        if next_char == \";\":\n",
    "            generated += \"\\n----------------------------------------\\n\"\n",
    "            \n",
    "    print(generated) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fe7191-aaf2-44f9-93c1-3c2b026226d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('myModel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2343e975-ba2e-4874-a953-1ec4242fd5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('myModel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ced3c2d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "generate_chars = 300\n",
    "temperature = 1.0\n",
    "start_index = random.randint(0, len(text) - seq_len - 1)\n",
    "generated = \"\"\n",
    "\n",
    "seed =  text[start_index : start_index + seq_len]\n",
    "    \n",
    "print('...Generating with seed: \"' + seed + '\"')\n",
    "\n",
    "    \n",
    "for i in range(generate_chars):\n",
    "    x_pred = np.zeros((1, len(seed), vocab_size))\n",
    "    for t, char in enumerate(seed):\n",
    "        x_pred[0, t, char_indices[char]] = 1\n",
    "    preds = model.predict(x_pred, verbose=0)[0]\n",
    "        \n",
    "    next_index = sample(preds, temperature)\n",
    "    next_char = indices_char[next_index]\n",
    "    seed = seed[1:] + next_char\n",
    "    generated += next_char\n",
    "        \n",
    "    if next_char == \";\":\n",
    "            generated += \"\\n----------------------------------------\\n\"\n",
    "            \n",
    "print(generated) \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bfe2ba",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf5075b-d863-43d8-bf7f-a40c2d9306c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
