{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cfb0df2-6691-42f0-a537-06a1ffbfd9a7",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc7d1ec-28ab-4353-9de5-9d0102b0d061",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d215e26-aa54-4cb1-985a-e117efb269a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /opt/conda/lib/python3.9/site-packages (4.2.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.9/site-packages (from gensim) (6.0.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.9/site-packages (from gensim) (1.8.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /opt/conda/lib/python3.9/site-packages (from gensim) (1.21.5)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "203148e5-af60-4988-b678-bda538d6249b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "from tqdm import tqdm\n",
    "import gensim\n",
    "from keras.callbacks import LambdaCallback\n",
    "\n",
    "# Deep learning: \n",
    "from keras.models import Input, Model\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d02a6e-d7e4-49bb-85a8-db2d70189bb6",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c39059e-77d5-42d6-bdce-766e49e9f5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datensatz einlesen\n",
    "df = pd.read_csv('data/out.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbd8388-e4bc-4b0b-b6f2-c8ab37360f92",
   "metadata": {},
   "source": [
    "## Transform dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f19816b9-d18a-4c52-bda3-7aef12cb0867",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataframe mit 3 Spalten. Werden so gejoint, dass ein neues Dataframe mit ein Haiku pro Zeile erstellt wird\n",
    "df = df[['0', '1', '2']].agg(lambda x: ' \\n '.join(x.values), axis=1)\n",
    "# Dataframe to list [[]] -> []\n",
    "haikus = df.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2726013-e79e-4d25-b5da-a0fc4e2c45f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['last red in the sky \\n a small girls moon face rises \\n over the counter', 'christmas services \\n a cellular phone rings out \\n handels messiah', 'passover darkness  \\n before the buds burst open \\n a childs eyes in death', 'last night of summer \\n the bright full moon of last night \\n hidden by a cloud', 'midnight and full moon \\n my neighbour asks to borrow \\n the vacum cleaner', 'yellow walnut leaves \\n slowly appear on the lawn \\n early morning light', 'after its first flight \\n the young gerfalcons talons \\n tighter on my glove', 'sultry afternoon \\n only the mailbox shadow \\n crosses the dirt road', 'long journey back home  \\n a forgotten bale of hay \\n slowly rots away', 'autumn mist obscures \\n the island in the distance \\n she cleans her glasses']\n",
      "number of haikus: 400000\n"
     ]
    }
   ],
   "source": [
    "# alle Haikus in Array\n",
    "print(haikus[:10])\n",
    "#number_of_haikus = len(haikus)\n",
    "number_of_haikus = 400000\n",
    "\n",
    "haikus = haikus[:number_of_haikus]\n",
    "print('number of haikus: ' + str(number_of_haikus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94a0b771-a466-4239-95a2-db1b428b1c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# die Haikus cleanen und selber auch noch mal als Wort-Arrays in großen Array\n",
    "haikus = np.array(haikus)\n",
    "\n",
    "def clean_and_split(sentence):\n",
    "    result = list(filter(''.__ne__, re.sub('[.,_]', '', sentence).split(' ')))\n",
    "    result.append(';')\n",
    "    return result\n",
    "\n",
    "haikus = list(map(lambda x: clean_and_split(x), haikus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bac24142-4e2f-43fd-aeea-6c8d3cf31769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['last', 'red', 'in', 'the', 'sky', '\\n', 'a', 'small', 'girls', 'moon', 'face', 'rises', '\\n', 'over', 'the', 'counter', ';'], ['christmas', 'services', '\\n', 'a', 'cellular', 'phone', 'rings', 'out', '\\n', 'handels', 'messiah', ';'], ['passover', 'darkness', '\\n', 'before', 'the', 'buds', 'burst', 'open', '\\n', 'a', 'childs', 'eyes', 'in', 'death', ';'], ['last', 'night', 'of', 'summer', '\\n', 'the', 'bright', 'full', 'moon', 'of', 'last', 'night', '\\n', 'hidden', 'by', 'a', 'cloud', ';'], ['midnight', 'and', 'full', 'moon', '\\n', 'my', 'neighbour', 'asks', 'to', 'borrow', '\\n', 'the', 'vacum', 'cleaner', ';'], ['yellow', 'walnut', 'leaves', '\\n', 'slowly', 'appear', 'on', 'the', 'lawn', '\\n', 'early', 'morning', 'light', ';'], ['after', 'its', 'first', 'flight', '\\n', 'the', 'young', 'gerfalcons', 'talons', '\\n', 'tighter', 'on', 'my', 'glove', ';'], ['sultry', 'afternoon', '\\n', 'only', 'the', 'mailbox', 'shadow', '\\n', 'crosses', 'the', 'dirt', 'road', ';'], ['long', 'journey', 'back', 'home', '\\n', 'a', 'forgotten', 'bale', 'of', 'hay', '\\n', 'slowly', 'rots', 'away', ';'], ['autumn', 'mist', 'obscures', '\\n', 'the', 'island', 'in', 'the', 'distance', '\\n', 'she', 'cleans', 'her', 'glasses', ';']]\n"
     ]
    }
   ],
   "source": [
    "print(haikus[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb02a0c-f7c6-4a67-b5c5-4fa75c5ab519",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## The word_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2343e975-ba2e-4874-a953-1ec4242fd5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train word_model\n",
    "word_model = gensim.models.Word2Vec(haikus, min_count=1) # min count automatisch groesser, discarded alle woerter die weniger vorkommen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d491045-c758-4929-aaf2-4cf375433183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec<vocab=43984, vector_size=100, alpha=0.025>\n"
     ]
    }
   ],
   "source": [
    "# summarize the loaded word_model\n",
    "print(word_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd1d78a9-21c3-4ce8-b2fa-3e8d6886af63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector for 'girl':\n",
      "[-1.7813144   1.390223   -0.04524625  0.44154942  1.0800593   0.26244986\n",
      "  0.15901282 -0.5032187   1.0188013   1.1947584   1.1067681   2.5262969\n",
      "  0.17680652 -1.217741   -3.147762   -0.2669855   0.78237444 -2.3427348\n",
      "  0.3931561  -0.9248451   0.8086693   0.26590395 -0.08549678 -1.3627003\n",
      "  0.3085851  -1.9268545  -2.2421246   0.17599379  0.51561797  1.269628\n",
      "  2.7472997   2.2557468   1.4816093   0.33526328 -0.656519    1.1573275\n",
      " -0.54194933  0.9562054  -1.2792095  -1.4622083   0.3493488  -1.009699\n",
      "  0.75311345 -0.95323175 -0.72239596 -0.23170902 -0.9226991   0.18376273\n",
      " -1.5672365   0.35558397 -0.07680202  1.5370051  -1.5546169   2.088508\n",
      "  1.7737682  -1.0869023   3.8877852   0.22606003 -0.99964124  0.4793532\n",
      "  1.7235538  -2.3479352   0.01460907  0.51211745 -0.50773156 -0.7129143\n",
      " -1.3684447  -1.4520518  -0.19157673 -1.3626063  -0.40733343 -0.08036793\n",
      " -0.30704677 -0.47070426 -1.1997057   1.0842602  -1.668591   -0.0764354\n",
      "  0.01873258  1.559697   -1.228483    1.253885    0.8940302   0.39928323\n",
      "  1.0684447   0.83297926 -0.49634713 -0.45011863 -0.32516488  0.02418184\n",
      " -1.3258265   0.4308677  -0.431039    0.02461405  2.6600764  -1.786061\n",
      "  0.07417028 -0.32673657 -0.21209878 -0.1791229 ]\n"
     ]
    }
   ],
   "source": [
    "# access vector for one word\n",
    "print('vector for \\'girl\\':')\n",
    "print(word_model.wv['girl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fd3d026-5e46-405a-ae8e-ebab45ee2075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10 words most similar to 'girl':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('woman', 0.8348810076713562),\n",
       " ('kid', 0.8027123212814331),\n",
       " ('guy', 0.7991846203804016),\n",
       " ('lady', 0.7637879848480225),\n",
       " ('chick', 0.7459321618080139),\n",
       " ('boy', 0.7285013794898987),\n",
       " ('girlfriend', 0.7156926393508911),\n",
       " ('bitch', 0.6953952312469482),\n",
       " ('sister', 0.6900787949562073),\n",
       " ('person', 0.687271237373352)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('top 10 words most similar to \\'girl\\':')\n",
    "word_model.wv.most_similar('girl', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8ef099b-640b-4e25-99a0-6468adce2efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity between 'go' and 'walk' (regarding the haikus):\n",
      "0.725224\n",
      "\n",
      "similarity between 'go' and 'laugh' (regarding the haikus):\n",
      "0.21716416\n",
      "\n",
      "similarity between 'go' and 'go':\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# similarity between two words\n",
    "print('similarity between \\'go\\' and \\'walk\\' (regarding the haikus):')\n",
    "print(word_model.wv.similarity(w1='go', w2='walk'))\n",
    "print()\n",
    "\n",
    "print('similarity between \\'go\\' and \\'laugh\\' (regarding the haikus):')\n",
    "print(word_model.wv.similarity(w1='go', w2='laugh'))\n",
    "print()\n",
    "\n",
    "print('similarity between \\'go\\' and \\'go\\':')\n",
    "print(word_model.wv.similarity(w1='go', w2='go'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef287aca-158d-47aa-a2ea-97b801155496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save word_model\n",
    "#word_model.save('w2v_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "108a5499-de48-4826-9fda-37a5cd5d8f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "#new_model = Word2Vec.load('w2v_model.bin')\n",
    "#print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb90c737-8d1a-4706-b013-f2a84b5950b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectors:\n",
      "[[ 0.3261586  -1.9390801  -0.8637559   0.60089767  0.8030032  -0.56526315\n",
      "   0.00928922  0.9588842   0.2725503   0.82826823 -1.4006827   0.6719266\n",
      "  -0.73364866  0.37084472 -0.851748   -1.9351178   2.5940175  -1.269681\n",
      "   0.57628906 -1.7919712  -0.98557836  0.5238613  -1.5037771  -0.6535714\n",
      "   0.36955866 -0.11089121  1.2329588  -0.7234964   0.8659579   0.17947729\n",
      "   2.1434555  -0.5281072   0.86789626 -0.22335638  0.05619117  0.8480278\n",
      "  -0.7587801   1.0685723   0.01354888  0.2048348   0.10864631 -0.21553792\n",
      "   0.7137529   1.5754474   0.5840277  -0.1095252  -0.7474503   0.61101633\n",
      "  -1.2372096   1.3347064   1.512253   -0.28660384 -1.0613586  -0.24530767\n",
      "  -1.1773177  -0.18630698  1.1211948  -0.7936223  -1.3308219  -0.6921227\n",
      "   0.620634    0.09585609 -1.9262667   1.4781322   0.5913094  -0.420123\n",
      "   1.3143097   0.3534205  -0.7647251  -0.97070414 -0.3863496  -0.25491062\n",
      "   0.9201879  -0.89660233 -1.0710522   1.5603441   0.0866755   0.27795032\n",
      "   0.2694539  -0.51851416  0.10907073 -1.0501305  -1.577848   -1.4186245\n",
      "  -0.69197744  1.2479247   0.22672552 -0.6900962   0.00788913 -0.5087358\n",
      "  -0.31206003 -0.01907419 -0.38221714 -1.8830559   1.6675518  -0.22430381\n",
      "  -0.29017562  0.41068128 -0.1921128  -0.3217739 ]\n",
      " [-0.96372795 -0.7529747  -0.5363877   0.06858172  0.44898862  0.56274503\n",
      "  -0.3420376   0.19402389 -0.8718884  -0.16686851 -0.09699951 -0.57568324\n",
      "  -0.19953102  1.7555808   0.04200461 -0.25966752  0.41056392 -0.1970708\n",
      "   0.20513006 -0.3048199   0.3189979  -1.608836    0.01832763 -1.0553077\n",
      "   0.6487532  -0.40914884  0.18128684  0.43700618 -0.43175605  0.71065557\n",
      "   1.0719687  -0.23865147  1.0584131  -0.3152209   0.88655096  0.40709406\n",
      "  -0.30761877  1.7317609  -0.47541147  1.0557677  -0.29301128 -0.6544975\n",
      "  -0.95423687  1.343399   -0.103039    1.6056851  -1.1671972   0.14693819\n",
      "   0.09754555  0.7815755  -0.27124238 -0.6619631   0.65144604 -0.20651779\n",
      "  -0.546464    0.03031648  0.13203833  0.3857488  -0.8275511   0.5647604\n",
      "   0.69165564 -0.46238917 -2.1749654  -0.8958056  -0.29603907  0.5007079\n",
      "   0.8812163   0.19560975 -0.9434295  -1.0558381  -0.12632151 -0.31962293\n",
      "   0.07354642 -0.2670152   0.25066686 -0.645834    0.13410297 -0.04895138\n",
      "   0.07732251 -0.9137267   0.19193032  0.03408182 -0.7192448  -0.41937673\n",
      "   0.17968568  0.9412741  -0.5610142  -0.6783323  -0.4382754   1.0954014\n",
      "  -0.0898509   0.16317733  0.6434139  -1.087754    0.2757771  -0.27864587\n",
      "  -0.21095656 -1.1825908  -0.843313    0.25623342]]\n",
      "\n",
      "labels:\n",
      "['\\n' ';' 'the' 'i' 'to' 'you' 'a' 'and' 'it' 'is']\n"
     ]
    }
   ],
   "source": [
    "# extract the words & their vectors, as numpy arrays\n",
    "vectors = np.asarray(word_model.wv.vectors)\n",
    "labels = np.asarray(word_model.wv.index_to_key)  # fixed-width numpy strings\n",
    "\n",
    "print('vectors:')\n",
    "print(vectors[:2])\n",
    "print()\n",
    "print('labels:')\n",
    "print(labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa22bfde-7705-43b1-a949-9610f087d26a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43984"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fde1864f-9255-4257-b256-3393ca0d312d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43984"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbf6e4f2-1ec2-4371-8158-7fb7879527d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://projector.tensorflow.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a668015a-c472-4633-b2f2-2b134fa7c817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metadata (labels) into tsv file\n",
    "pd.DataFrame(labels).to_csv(\"model_dir/metadata.tsv\", sep = '\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5e66453-e7ed-49b0-99c3-7cbb96887765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save vectors into tsv file\n",
    "pd.DataFrame(vectors).to_csv(\"model_dir/vectors.tsv\", sep = '\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962ce0f1-52c7-4c0f-986e-3dc9b46dec9b",
   "metadata": {},
   "source": [
    "## Creating Model for HaikuGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b257feb1-587f-416d-8629-8794d3f8c05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximale Anzahl der Wörter in einem Haiku aus Datenset\n",
    "max_haiku_len = len(max(haikus, key=len))\n",
    "max_features = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ca2faec-c14d-49e3-b4e8-a91829318694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['midnight',\n",
       " 'and',\n",
       " 'full',\n",
       " 'moon',\n",
       " '\\n',\n",
       " 'my',\n",
       " 'neighbour',\n",
       " 'asks',\n",
       " 'to',\n",
       " 'borrow',\n",
       " '\\n',\n",
       " 'the',\n",
       " 'vacum',\n",
       " 'cleaner',\n",
       " ';']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "haikus[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24c29a53-dd37-4eab-b5c8-0786d1166a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.5820458e+00,  2.8715370e+00, -2.0762448e+00, -1.2740515e-01,\n",
       "        2.8581939e+00, -1.9330012e+00,  3.7553601e-02,  2.6797233e+00,\n",
       "        2.5666494e+00,  2.3984778e+00, -4.8610780e-02,  2.7564251e+00,\n",
       "        7.6587105e-01, -1.4019598e+00,  2.0969794e+00, -1.3246665e+00,\n",
       "       -5.6277877e-01,  2.8978521e-01,  1.1375366e-03,  1.0887215e+00,\n",
       "       -2.8589797e+00,  1.2886252e+00,  1.8312030e+00,  1.4780592e+00,\n",
       "       -1.1087697e+00, -9.4945067e-01, -1.2071359e+00,  1.1930063e+00,\n",
       "       -1.3305489e+00, -1.2799729e+00,  1.5982162e+00,  1.7785784e+00,\n",
       "        2.2997376e-01, -6.0743862e-01, -9.5628017e-01,  2.5486543e+00,\n",
       "        9.0234995e-02, -4.8573482e-01, -9.4896585e-01,  2.0705674e+00,\n",
       "       -2.4946468e+00, -4.2541456e-01,  1.9732209e+00, -3.5095584e-02,\n",
       "       -2.1348510e+00,  2.0176039e+00, -6.7293227e-01, -9.1287255e-02,\n",
       "        6.3855618e-01,  1.9062890e+00,  5.2674782e-01, -6.0647037e-02,\n",
       "       -2.6684752e+00, -1.4846808e+00, -1.3958681e+00, -1.3744413e+00,\n",
       "       -4.3846428e-01, -3.0027574e-01, -5.1083404e-01, -1.4179147e+00,\n",
       "        8.4380321e-02,  1.3352807e+00, -1.0932319e+00,  3.2069969e+00,\n",
       "       -8.9259958e-01, -6.1853904e-01,  2.8824219e-01,  1.4709993e-01,\n",
       "        5.5851424e-01,  1.6735786e+00,  2.2697029e+00, -1.0312995e+00,\n",
       "       -2.5721824e+00, -1.4177531e+00,  2.5871176e-01, -8.9439857e-01,\n",
       "        1.7384824e-01,  1.0206059e+00,  8.5627325e-02, -4.9487594e-01,\n",
       "       -4.0077653e+00, -5.8402067e-01, -1.1790365e+00, -5.2741461e+00,\n",
       "        6.0300070e-01,  3.3432956e+00, -2.3893285e-01,  2.5101874e+00,\n",
       "       -1.4115772e+00, -1.4693668e+00, -1.2895461e+00, -6.8309510e-01,\n",
       "       -1.3456655e+00, -1.1733387e+00, -1.0227557e+00, -4.2378068e+00,\n",
       "       -1.0876698e+00,  7.0608997e-01, -4.1434059e+00,  3.9904749e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_model.wv[\"out\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4d81a12-52e9-460b-9de6-b13a56a8b1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2idx(word):\n",
    "    return word_model.wv.key_to_index[word]\n",
    "def idx2word(idx):\n",
    "    return word_model.wv.index_to_key[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d63714d6-b354-48c2-bce6-b322b37de08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['red', 'in', 'the', 'sky', '\\n', 'a', 'small', 'girls', 'moon', 'face', 'rises', '\\n', 'over', 'the', 'counter', ';', 'christmas', 'services', '\\n']\n",
      "Number of sequences: 910379\n",
      "train_x shape: (910379, 20)\n",
      "train_y shape: (910379,)\n"
     ]
    }
   ],
   "source": [
    "#haikus_combined =  sum(haikus, [])\n",
    "\n",
    "haikus_combined = [item for sublist in haikus for item in sublist]\n",
    "# cut the text in semi-redundant sequences of seq_len characters\n",
    "print(haikus_combined[1:20])\n",
    "seq_len = max_haiku_len\n",
    "step = 7\n",
    "# Input String\n",
    "sequences = []\n",
    "#Output character\n",
    "next_words = []\n",
    "for i in range(0, len(haikus_combined) - seq_len, step):\n",
    "    sequences.append(haikus_combined[i : i + seq_len])\n",
    "    next_words.append(haikus_combined[i + seq_len])\n",
    "print(\"Number of sequences:\", len(sequences))\n",
    "\n",
    "train_x = np.zeros([len(sequences), max_haiku_len], dtype=np.int32)\n",
    "train_y = np.zeros([len(next_words)], dtype=np.int32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, word in enumerate(sequence):\n",
    "         train_x[i, t] = word2idx(word)\n",
    "    train_y[i] = word2idx(next_words[i])\n",
    "print('train_x shape:', train_x.shape)\n",
    "print('train_y shape:', train_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "005209b8-d549-465b-9c84-be781600d359",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('\\nPreparing the data for LSTM...')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#train_x = np.zeros([len(haikus), max_haiku_len], dtype=np.int32)\n",
    "#train_y = np.zeros([len(haikus)], dtype=np.int32)\n",
    "#for i, haiku in enumerate(haikus):\n",
    "#    for t, word in enumerate(haiku[:-1]):\n",
    "#        train_x[i, t] = word2idx(word)\n",
    "#    train_y[i] = word2idx(haiku[-1])\n",
    "#print('train_x shape:', train_x.shape)\n",
    "#print('train_y shape:', train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2dd02369-d7cd-4e41-9220-79c229c8fb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result embedding shape: (43984, 100)\n"
     ]
    }
   ],
   "source": [
    "pretrained_weights = word_model.wv.vectors\n",
    "vocab_size = pretrained_weights.shape[0]\n",
    "emdedding_size = pretrained_weights.shape[1]\n",
    "print('Result embedding shape:', pretrained_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "121ed410-c97d-47cf-849d-9464ccee1020",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Activation \n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c62ea891-6506-4d4c-8b83-6733a6a14d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-10 11:07:03.228812: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-10 11:07:03.271230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-10 11:07:03.271808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-10 11:07:03.272921: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-10 11:07:03.273911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-10 11:07:03.274402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-10 11:07:03.274829: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-10 11:07:04.265900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-10 11:07:04.266428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-10 11:07:04.266881: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-10 11:07:04.267218: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13696 MB memory:  -> device: 0, name: GRID V100-16Q, pci bus id: 0000:06:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=emdedding_size, weights=[pretrained_weights]))\n",
    "model.add(LSTM(emdedding_size, dropout=0.2, recurrent_dropout=0.2,return_sequences=True))\n",
    "model.add(LSTM(emdedding_size*2, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(vocab_size)),\n",
    "model.add(Activation('softmax')),\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "#print('\\nTraining LSTM...')\n",
    "#model = Sequential()\n",
    "#model.add(Embedding(input_dim=vocab_size, output_dim=emdedding_size, weights=[pretrained_weights]))\n",
    "#model.add(LSTM(units=emdedding_size))\n",
    "#model.add(Dense(units=vocab_size))\n",
    "#model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "13eacbe7-335b-4dcb-a9f9-aaa8093eba92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 100)         4398400   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, None, 100)         80400     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 200)               240800    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 43984)             8840784   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 43984)             0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,560,384\n",
      "Trainable params: 13,560,384\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "926f1a9c-643c-45d0-be80-54aead671437",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.fit(train_x, train_y,\n",
    "#          batch_size=512,\n",
    "#          epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "127bd25e-7303-4e8d-9807-8d9b014e4f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    if temperature <= 0:\n",
    "        return np.argmax(preds)\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "40f48de5-a9e5-442e-af79-24436044a788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next(text, num_generated=10):\n",
    "    word_idxs = [word2idx(word) for word in text.lower().split()]\n",
    "    for i in range(num_generated):\n",
    "        prediction = model.predict(x=np.array(word_idxs))\n",
    "        idx = sample(prediction[-1], temperature=0.7)\n",
    "        word_idxs.append(idx)\n",
    "    return ' '.join(idx2word(idx) for idx in word_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9db2aeea-8a22-416d-893f-ba57b694bba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_haikus(generate_words, temperature=1.0):\n",
    "\n",
    "    start_index = random.randint(0, train_x.shape[0] - seq_len - 1)\n",
    "    generated = \"\"\n",
    "\n",
    "    seed =  train_x[start_index:start_index+1]\n",
    "\n",
    "    for i in range(generate_words):\n",
    "        preds = model.predict(seed)\n",
    "        pred_index = sample(preds[-1])\n",
    "        pred_word = idx2word(pred_index)\n",
    "    \n",
    "    \n",
    "        seed_tmp = np.concatenate((seed[0][1:], [pred_index]))\n",
    "        seed[0] = seed_tmp\n",
    "    \n",
    "        generated += pred_word + \" \"\n",
    "    \n",
    "        if pred_word == \";\":\n",
    "            generated += \"\\n----------------------------------------\\n\"\n",
    "\n",
    "    print(generated)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "12613068-af8c-454b-99d3-d9c325b6b68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "EPOCH:0\n",
      "890/890 [==============================] - 259s 285ms/step - loss: 6.0999\n",
      "is \n",
      " ever happy the made practically reviews really \n",
      " dude thinks your carry ; \n",
      "----------------------------------------\n",
      "off considered ; \n",
      "----------------------------------------\n",
      "how i agree we play \n",
      " heavy bitches to with black \n",
      " \n",
      "\n",
      "\n",
      "EPOCH:1\n",
      "890/890 [==============================] - 249s 280ms/step - loss: 5.1814\n",
      "dad know ; \n",
      "----------------------------------------\n",
      "i know a a naked \n",
      " seem to mean up however \n",
      " id living pretty ; \n",
      "----------------------------------------\n",
      "visiting level \n",
      " is your venue but you am \n",
      " \n",
      " \n",
      "\n",
      "\n",
      "EPOCH:2\n",
      "890/890 [==============================] - 254s 286ms/step - loss: 4.8883\n",
      "to vote \n",
      " for laughing but they can want \n",
      " to give to rain up ; \n",
      "----------------------------------------\n",
      "if it can i \n",
      " look great th from all which \n",
      " you understand \n",
      "\n",
      "\n",
      "EPOCH:3\n",
      "890/890 [==============================] - 254s 286ms/step - loss: 4.7353\n",
      "isnt so much \n",
      " late things in shot here ; \n",
      "----------------------------------------\n",
      "im contained of \n",
      " them and cant prove you smile to \n",
      " increase and would move ; \n",
      "----------------------------------------\n",
      "deleted believe \n",
      " \n",
      "\n",
      "\n",
      "EPOCH:4\n",
      "890/890 [==============================] - 246s 276ms/step - loss: 4.6260\n",
      "was \n",
      " how like it has grown metal \n",
      " responses i think ; \n",
      "----------------------------------------\n",
      "yeah a best mod is \n",
      " be funny enough i think \n",
      " youre amazing ; \n",
      "----------------------------------------\n",
      "the disgrace \n",
      "\n",
      "\n",
      "EPOCH:5\n",
      "890/890 [==============================] - 243s 273ms/step - loss: 4.5393\n",
      "that i dont say reading how \n",
      " you stay for wilder ; \n",
      "----------------------------------------\n",
      "oh dont you have more \n",
      " things at all an dumbest rate \n",
      " you even mean lol ; \n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "EPOCH:6\n",
      " 14/890 [..............................] - ETA: 3:57 - loss: 4.4528"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEPOCH:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m          \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmyModelEmbedding.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m generate_haikus(\u001b[38;5;241m30\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1379\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   1380\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m   1381\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1382\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1383\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1384\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1385\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1386\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2953\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2954\u001b[0m   (graph_function,\n\u001b[1;32m   2955\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2956\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1849\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1851\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1852\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1853\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1855\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m     args,\n\u001b[1;32m   1857\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1858\u001b[0m     executing_eagerly)\n\u001b[1;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 512\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print()\n",
    "    print()\n",
    "    print(f\"EPOCH:{epoch}\")\n",
    "    model.fit(train_x, train_y, \n",
    "              batch_size=batch_size, \n",
    "              epochs=1)\n",
    "    \n",
    "    model.save('myModelEmbedding.h5')\n",
    "\n",
    "    generate_haikus(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "75c929fa-14f7-45f3-9357-47b521859eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn to live on ; \n",
      "----------------------------------------\n",
      "again what that is \n",
      " introduced it is so much \n",
      " i cant be deep cheap ; \n",
      "----------------------------------------\n",
      "i want opinion \n",
      " with the golden bus were friends \n",
      " for drugs with the sky ; \n",
      "----------------------------------------\n",
      "you dont know when you \n",
      " cant try anything on them \n",
      " for usual this clown ; \n",
      "----------------------------------------\n",
      "you met epic run \n",
      " ill my flight a lot for what \n",
      " asked you for all yet ; \n",
      "----------------------------------------\n",
      "but it is exactly \n",
      " what they were commenting from \n",
      " the world to save soft ; \n",
      "----------------------------------------\n",
      "hes not saying bubbles \n",
      " sunglasses here p bounce so \n",
      " far hype ovation soon ; \n",
      "----------------------------------------\n",
      "was a lovely can \n",
      " be able to tell you on \n",
      " my hardware ask you ; \n",
      "----------------------------------------\n",
      "can read feel on her \n",
      " to see or start obsessing \n",
      " on everyone else ; \n",
      "----------------------------------------\n",
      "colorado with \n",
      " boxes doing the rifle \n",
      " from thing for pregnant ; \n",
      "----------------------------------------\n",
      "and just looks like bad \n",
      " the question is if i cant \n",
      " be high for forever ; \n",
      "----------------------------------------\n",
      "what do you want it \n",
      " a lot of solid issues \n",
      " very much free friends ; \n",
      "----------------------------------------\n",
      "two there are a time \n",
      " for hoping i step on \n"
     ]
    }
   ],
   "source": [
    "generate_haikus(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3f9a7f-b5a1-4267-a7a5-58cb550058a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
