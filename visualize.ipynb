{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /opt/conda/lib/python3.9/site-packages (4.2.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.9/site-packages (from gensim) (6.0.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /opt/conda/lib/python3.9/site-packages (from gensim) (1.21.5)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.9/site-packages (from gensim) (1.8.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q gpt-2-simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-20 14:14:20.937728: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-20 14:14:20.937779: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import ipywidgets as widgets\n",
    "import gensim\n",
    "import time\n",
    "import gpt_2_simple as gpt2\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Datensatz einlesen\n",
    "df = pd.read_csv('data/out.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Dataframe mit 3 Spalten. Werden so gejoint, dass ein neues Dataframe mit ein Haiku pro Zeile erstellt wird\n",
    "df = df[['0', '1', '2']].agg(lambda x: '\\n'.join(x.values), axis=1)\n",
    "# Dataframe to list [[]] -> []\n",
    "array_of_poems = df.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# num_poems an Haikus mit Semicolon zwshceneinander zusammenfuegen\n",
    "num_poems = 10000\n",
    "\n",
    "text = ';'.join(array_of_poems[:num_poems])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 unique chars\n",
      "['\\n', ' ', ';', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# Anzahl der unterschielichen characters im gesamt datensatz herausfinden\n",
    "vocab = sorted(set(text))\n",
    "vocab_size = len(vocab)\n",
    "print(f'{vocab_size} unique chars')\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Dictionary erstellen. Jeder character wird mit einer Zahl nummeriert\n",
    "char_indices = {c: i for i, c in enumerate(vocab)}\n",
    "indices_char = {i: c for i, c in enumerate(vocab)}\n",
    "\n",
    "# cut the text in semi-redundant sequences of seq_len characters\n",
    "seq_len = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-20 14:14:28.434104: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-06-20 14:14:28.434154: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-06-20 14:14:28.434181: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (jupyter-acs277): /proc/driver/nvidia/version does not exist\n",
      "2022-06-20 14:14:28.434462: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model('myModelTuner.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = keras.models.load_model('myModelEmbedding.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Standartfunktion Probability array to onehot to integerencoded char\n",
    "# [0.3, 0.2, 0.1, 0.4] -> [0, 0, 0, 1] -> return 4 (stelle, an der 1)\n",
    "def sample(prob, temperature=1.0):\n",
    "    np.seterr(divide = 'ignore')\n",
    "    #np.seterr(divide = 'warn') \n",
    "    # helper function to sample an index from a probability array\n",
    "    prob = np.asarray(prob).astype(\"float64\")\n",
    "    prob = np.log(prob) / temperature\n",
    "    exp_prob = np.exp(prob)\n",
    "    prob = exp_prob / np.sum(exp_prob)\n",
    "    probas = np.random.multinomial(1, prob)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "output = widgets.Output()\n",
    "@output.capture(clear_output=True)\n",
    "def generate_haiku_characterbased(haiku_start, temperature=1.0):\n",
    "    lenString = 300\n",
    "    randomNum = random.randint(0, len(text) - lenString)\n",
    "    findTemplate = text[randomNum: randomNum + lenString]\n",
    "\n",
    "    pattern = '(.{99};)'\n",
    "    seed = re.search(pattern, findTemplate, re.DOTALL).group()\n",
    "\n",
    "    poem = format(haiku_start.value)\n",
    "    seed = seed[len(poem):] + poem\n",
    "\n",
    "\n",
    "    print(poem, end='')\n",
    "    \n",
    "    while(len(poem) == 0 or poem[-1] != ';'):\n",
    "        x_pred = np.zeros((1, len(seed), vocab_size))\n",
    "        for t, char in enumerate(seed):\n",
    "            x_pred[0, t, char_indices[char]] = 1\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds, temperature)\n",
    "        next_char = indices_char[next_index]\n",
    "        seed = seed[1:] + next_char\n",
    "        poem += next_char\n",
    "        time.sleep(200/1000)\n",
    "        if(next_char != ';'):\n",
    "            print(next_char, end='')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('data/out.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe mit 3 Spalten. Werden so gejoint, dass ein neues Dataframe mit ein Haiku pro Zeile erstellt wird\n",
    "df2 = df2[['0', '1', '2']].agg(lambda x: ' \\n '.join(x.values), axis=1)\n",
    "# Dataframe to list [[]] -> []\n",
    "haikus = df2.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of haikus: 10000\n"
     ]
    }
   ],
   "source": [
    "# alle Haikus in Array\n",
    "#number_of_haikus = len(haikus)\n",
    "number_of_haikus = 10000\n",
    "\n",
    "haikus = haikus[:number_of_haikus]\n",
    "\n",
    "\n",
    "print('number of haikus: ' + str(number_of_haikus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# die Haikus cleanen und selber auch noch mal als Wort-Arrays in gro√üen Array\n",
    "haikus = np.array(haikus)\n",
    "\n",
    "def clean_and_split(sentence):\n",
    "    result = list(filter(''.__ne__, re.sub('[.,_]', '', sentence).split(' ')))\n",
    "    result.append(';')\n",
    "    return result\n",
    "\n",
    "haikus = list(map(lambda x: clean_and_split(x), haikus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model = gensim.models.Word2Vec.load('w2v_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2idx(word):\n",
    "    return word_model.wv.key_to_index[word]\n",
    "def idx2word(idx):\n",
    "    return word_model.wv.index_to_key[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9998\n"
     ]
    }
   ],
   "source": [
    "#haikus_combined =  sum(haikus, [])\n",
    "\n",
    "haikus_combined = [item for sublist in haikus for item in sublist]\n",
    "# cut the text in semi-redundant sequences of seq_len characters\n",
    "seq_len = 20\n",
    "step = 1\n",
    "# Input String\n",
    "sequences = []\n",
    "\n",
    "for i in range(0, len(haikus_combined) - seq_len, step):\n",
    "    sequences.append(haikus_combined[i : i + seq_len])\n",
    "\n",
    "train_x = np.zeros([len(sequences), seq_len], dtype=np.int32)\n",
    "\n",
    "\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, word in enumerate(sequence):\n",
    "         train_x[i, t] = word2idx(word)\n",
    "\n",
    "\n",
    "\n",
    "generation = []\n",
    "for i, sequence in enumerate(train_x):\n",
    "    if(train_x[i, seq_len- 1] == word2idx(\";\")):\n",
    "        generation.append(sequence)\n",
    "        \n",
    "print(len(generation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample2(preds, temperature=1.0):\n",
    "    np.seterr(divide = 'ignore')\n",
    "    #np.seterr(divide = 'warn') \n",
    "    if temperature <= 0:\n",
    "        return np.argmax(preds)\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next(text, num_generated=10):\n",
    "    word_idxs = [word2idx(word) for word in text.lower().split()]\n",
    "    for i in range(num_generated):\n",
    "        prediction = model.predict(x=np.array(word_idxs))\n",
    "        idx = sample(prediction[-1], temperature=0.7)\n",
    "        word_idxs.append(idx)\n",
    "    return ' '.join(idx2word(idx) for idx in word_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "output2 = widgets.Output()\n",
    "@output2.capture(clear_output=True)\n",
    "def generate_haiku_wordbased(haiku_start, temperature=1.0):\n",
    "    \n",
    "    haiku_start = format(haiku_start.value)\n",
    "    \n",
    "    split_words = list(filter(''.__ne__, re.sub('[.,_]', '', haiku_start).split(' ')))\n",
    "    \n",
    "    start_index = random.randint(0, len(generation)-1)\n",
    "    generate_with =  generation[start_index]\n",
    "    \n",
    "    for word in split_words:\n",
    "        generate_with = np.append(generate_with[1:], word2idx(word))\n",
    "\n",
    "\n",
    "    seed = np.zeros([1, seq_len], dtype=np.int32)\n",
    "    for i in range(seq_len):\n",
    "        seed[0][i] = generate_with[i]\n",
    "\n",
    "    pred_word = \"\"\n",
    "    print(haiku_start, end=' ')\n",
    "    while(pred_word != \";\"):\n",
    "        preds = model2.predict(seed)\n",
    "        pred_index = sample2(preds[-1])\n",
    "        pred_word = idx2word(pred_index)\n",
    "        seed_tmp = np.concatenate((seed[0][1:], [pred_index]))\n",
    "        seed[0] = seed_tmp\n",
    "        time.sleep(500/1000)\n",
    "        if(pred_word != \";\"):\n",
    "            print(pred_word, end=' ')\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint checkpoint/run1/model-1000\n",
      "INFO:tensorflow:Restoring parameters from checkpoint/run1/model-1000\n"
     ]
    }
   ],
   "source": [
    "#sess = gpt2.start_tf_sess()\n",
    "sess = gpt2.reset_session(sess)\n",
    "gpt2.load_gpt2(sess, run_name='run1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "output3 = widgets.Output()\n",
    "@output3.capture(clear_output=True)\n",
    "def generate_haiku_transformer(haiku_start, temperature=1.0):\n",
    "    haiku_start_format = format(haiku_start.value)\n",
    "    \n",
    "    lenString = 300\n",
    "    randomNum = random.randint(0, len(text) - lenString)\n",
    "    findTemplate = text[randomNum: randomNum + lenString]\n",
    "    \n",
    "    pattern = ';([^;]*;)'\n",
    "    seed = re.search(pattern, findTemplate, re.DOTALL).group(1)\n",
    "    \n",
    "    generated_transformer = gpt2.generate(sess,\n",
    "              length=30,\n",
    "              temperature=1.0,\n",
    "              prefix=seed + \"\\n\\n\" + haiku_start_format,\n",
    "              nsamples=1,\n",
    "              batch_size=1,\n",
    "              return_as_list=True)[0]\n",
    "    pattern_transformer = ';\\n\\n([^;]*);'\n",
    "    haiku_transformer = re.search(pattern_transformer, generated_transformer, re.DOTALL).group(1)\n",
    "    print(haiku_transformer)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b034802b73534270853d40c82b730af1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='Generate a characterbased Haiku:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c9ef4d1bb7f4095b448f4b55962bd24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', placeholder='Type in the start of your Haiku...')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc149d376ba148a5981b34db2c4372be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_haiku = widgets.Text(value=\"\",\n",
    "                           placeholder='Type in the start of your Haiku...',\n",
    "                           disable=False\n",
    ")\n",
    "\n",
    "description = widgets.HTML('Generate a characterbased Haiku:')\n",
    "\n",
    "display(description)\n",
    "display(input_haiku)\n",
    "\n",
    "input_haiku.on_submit(generate_haiku_characterbased)\n",
    "display(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c80fbd88f955437f93c07a15778f9acb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='Generate a word-based Haiku:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0e13dc5a3644955a155e8526de16bc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', placeholder='Type in the start of your Haiku...')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef710bd0e9d341cfb300712057597b3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_haiku = widgets.Text(value=\"\",\n",
    "                           placeholder='Type in the start of your Haiku...',\n",
    "                           disable=False\n",
    ")\n",
    "\n",
    "description = widgets.HTML('Generate a word-based Haiku:')\n",
    "\n",
    "display(description)\n",
    "display(input_haiku)\n",
    "\n",
    "input_haiku.on_submit(generate_haiku_wordbased)\n",
    "display(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e23b2bf88a124294bd4ab089ca7e9c62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='Generate Transformer-Haiku:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56987c2760164912a920c63cf8059ae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', placeholder='Type in the start of your Haiku...')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abcdb34add3348f6bc7b2b22be193c5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_haiku = widgets.Text(value=\"\",\n",
    "                           placeholder='Type in the start of your Haiku...',\n",
    "                           disable=False\n",
    ")\n",
    "\n",
    "description = widgets.HTML('Generate Transformer-Haiku:')\n",
    "\n",
    "display(description)\n",
    "display(input_haiku)\n",
    "\n",
    "input_haiku.on_submit(generate_haiku_transformer)\n",
    "display(output3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
